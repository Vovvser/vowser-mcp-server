# Neo4j ë°ì´í„°ë² ì´ìŠ¤ ì•„í‚¤í…ì²˜ ì„¤ê³„ ë¬¸ì„œ

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ë°ì´í„° ëª¨ë¸ë§ ì² í•™](#ë°ì´í„°-ëª¨ë¸ë§-ì² í•™)
3. [ë…¸ë“œ íƒ€ì… ìƒì„¸ ì„¤ê³„](#ë…¸ë“œ-íƒ€ì…-ìƒì„¸-ì„¤ê³„)
4. [ê´€ê³„ êµ¬ì¡° ìƒì„¸ ì„¤ê³„](#ê´€ê³„-êµ¬ì¡°-ìƒì„¸-ì„¤ê³„)
5. [ì¸ë±ìŠ¤ ë° ì œì•½ ì¡°ê±´](#ì¸ë±ìŠ¤-ë°-ì œì•½-ì¡°ê±´)
6. [Section ìë™ ìƒì„± ì‹œìŠ¤í…œ](#section-ìë™-ìƒì„±-ì‹œìŠ¤í…œ)
7. [í˜ì´ì§• ì²˜ë¦¬ ì•„í‚¤í…ì²˜](#í˜ì´ì§•-ì²˜ë¦¬-ì•„í‚¤í…ì²˜)
8. [ë°ì´í„° ì €ì¥ ìµœì í™”](#ë°ì´í„°-ì €ì¥-ìµœì í™”)
9. [ì¿¼ë¦¬ íŒ¨í„´ ë° ì„±ëŠ¥](#ì¿¼ë¦¬-íŒ¨í„´-ë°-ì„±ëŠ¥)
10. [í™•ì¥ì„± ê³ ë ¤ì‚¬í•­](#í™•ì¥ì„±-ê³ ë ¤ì‚¬í•­)

---

## ê°œìš”

### ğŸ¯ ëª©ì 
ì›¹ì‚¬ì´íŠ¸ì˜ ê³„ì¸µì  êµ¬ì¡°ë¥¼ ê·¸ë˜í”„ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ëª¨ë¸ë§í•˜ì—¬ ì†ì„ ì‚¬ìš©í•˜ê¸° ì–´ë ¤ìš´ ì‚¬ìš©ìë“¤ì´ íš¨ìœ¨ì ìœ¼ë¡œ ì›¹ì‚¬ì´íŠ¸ë¥¼ íƒìƒ‰í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ëŠ” ì‹œìŠ¤í…œì˜ ë°ì´í„° ì•„í‚¤í…ì²˜

### ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ
- **ë°ì´í„°ë² ì´ìŠ¤**: Neo4j 5.x
- **ì¿¼ë¦¬ ì–¸ì–´**: Cypher
- **ì—°ê²° ë¼ì´ë¸ŒëŸ¬ë¦¬**: LangChain-Neo4j
- **í”„ë¡œê·¸ë˜ë° ì–¸ì–´**: Python 3.11

### ğŸ“Š ë°ì´í„° ëª¨ë¸ë§ ì›ì¹™
1. **ê³„ì¸µì  êµ¬ì¡°**: ì›¹ì‚¬ì´íŠ¸ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ê³„ì¸µ êµ¬ì¡° ë°˜ì˜
2. **ê´€ê³„ ì¤‘ì‹¬**: ë…¸ë“œ ê°„ì˜ ê´€ê³„ë¥¼ í†µí•œ íƒìƒ‰ ê²½ë¡œ ì œê³µ
3. **í™•ì¥ì„±**: ëŒ€ìš©ëŸ‰ ì›¹ì‚¬ì´íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥í•œ êµ¬ì¡°
4. **ì ‘ê·¼ì„±**: ì‚¬ìš©ì ì¹œí™”ì ì¸ íƒìƒ‰ ì •ë³´ ì œê³µ
5. **ì§€ëŠ¥í˜• ê²€ìƒ‰**: textToCypherì™€ embedding ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰ ì§€ì›
6. **ìì—°ì–´ ì§ˆì˜**: ìì—°ì–´ë¥¼ Cypher ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ëŠ¥

---

## ë°ì´í„° ëª¨ë¸ë§ ì² í•™

### ğŸ§  ì„¤ê³„ ì² í•™

#### 1. **ê³„ì¸µì  ì›¹ êµ¬ì¡° ë°˜ì˜**
```
ì›¹ì‚¬ì´íŠ¸ ì‹¤ì œ êµ¬ì¡°    â†’    Neo4j ë…¸ë“œ êµ¬ì¡°
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Domain               â†’    ROOT
â”œâ”€ Subdomain         â†’    SUBROOT
â”œâ”€ Page (depth=1)    â†’    DEPTH
â”‚  â”œâ”€ Header         â†’    SECTION
â”‚  â”œâ”€ Navigation     â†’    SECTION
â”‚  â”œâ”€ Content        â†’    SECTION
â”‚  â”‚  â”œâ”€ Article     â†’    ELEMENT
â”‚  â”‚  â”‚  â”œâ”€ Title    â†’    CONTENT
â”‚  â”‚  â”‚  â””â”€ Link     â†’    CONTENT
â”‚  â”‚  â””â”€ Pagination  â†’    SECTION (special)
â”‚  â””â”€ Footer         â†’    SECTION
â””â”€ Page (depth=2)    â†’    DEPTH
```

#### 2. **ê´€ê³„ ì¤‘ì‹¬ íƒìƒ‰**
- ëª¨ë“  ë…¸ë“œëŠ” ëª…í™•í•œ ê´€ê³„ë¥¼ í†µí•´ ì—°ê²°
- íƒìƒ‰ ê²½ë¡œ ì¶”ì  ê°€ëŠ¥
- ì—­ë°©í–¥ íƒìƒ‰ ì§€ì›

#### 3. **ë©”íƒ€ë°ì´í„° ì¤‘ì‹¬ ì„¤ê³„**
- ê° ë…¸ë“œì— í’ë¶€í•œ ë©”íƒ€ë°ì´í„° ì €ì¥
- AI ìš”ì•½ì„ í†µí•œ ì ‘ê·¼ì„± í–¥ìƒ
- ì‚¬ìš©ì ë§ì¶¤í˜• ì •ë³´ ì œê³µ

---

## ë…¸ë“œ íƒ€ì… ìƒì„¸ ì„¤ê³„

### 1. ROOT ë…¸ë“œ - ìµœìƒìœ„ ë„ë©”ì¸

#### ğŸ¯ ëª©ì 
ì›¹ì‚¬ì´íŠ¸ì˜ ìµœìƒìœ„ ë„ë©”ì¸ì„ ëŒ€í‘œí•˜ëŠ” ë…¸ë“œ

#### ğŸ“Š ìŠ¤í‚¤ë§ˆ
```cypher
(:ROOT {
  url: String,              // ì™„ì „í•œ URL (https://www.example.com)
  domain: String,           // ë„ë©”ì¸ë§Œ (example.com)
  title: String,            // ì›¹ì‚¬ì´íŠ¸ ì œëª©
  description: String,      // ì›¹ì‚¬ì´íŠ¸ ì„¤ëª…
  favicon_url: String,      // íŒŒë¹„ì½˜ URL (ì„ íƒì‚¬í•­)
  language: String,         // ì£¼ì–¸ì–´ (ko, en, etc.)
  created_at: DateTime,     // ìƒì„± ì‹œê°„
  updated_at: DateTime,     // ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„
  crawl_status: String,     // "completed", "in_progress", "failed"
  total_pages: Integer,     // ì´ í¬ë¡¤ë§ëœ í˜ì´ì§€ ìˆ˜
  crawl_depth: Integer,     // í¬ë¡¤ë§ ê¹Šì´
  last_crawled: DateTime    // ë§ˆì§€ë§‰ í¬ë¡¤ë§ ì‹œê°„
})
```

#### ğŸ” ì‚¬ìš© ì˜ˆì‹œ
```cypher
// ROOT ë…¸ë“œ ìƒì„±
CREATE (root:ROOT {
  url: "https://www.naver.com",
  domain: "naver.com",
  title: "NAVER",
  description: "ëŒ€í•œë¯¼êµ­ ìµœëŒ€ í¬í„¸ì‚¬ì´íŠ¸",
  language: "ko",
  created_at: datetime(),
  updated_at: datetime(),
  crawl_status: "completed",
  total_pages: 156,
  crawl_depth: 3,
  last_crawled: datetime()
})
```

#### ğŸš€ ìµœì í™” ê³ ë ¤ì‚¬í•­
- `domain` í•„ë“œì— ìœ ë‹ˆí¬ ì œì•½ ì¡°ê±´ ì ìš©
- `url` í•„ë“œì— ì¸ë±ìŠ¤ ìƒì„±
- `crawl_status` í•„ë“œë¡œ í¬ë¡¤ë§ ìƒíƒœ ê´€ë¦¬

---

### 2. SUBROOT ë…¸ë“œ - ì„œë¸Œë„ë©”ì¸

#### ğŸ¯ ëª©ì 
ë©”ì¸ ë„ë©”ì¸ê³¼ ë‹¤ë¥¸ ì„œë¸Œë„ë©”ì¸ì´ë‚˜ ì™¸ë¶€ ë§í¬ë¥¼ ê´€ë¦¬

#### ğŸ“Š ìŠ¤í‚¤ë§ˆ
```cypher
(:SUBROOT {
  url: String,              // ì„œë¸Œë„ë©”ì¸ URL
  parent_domain: String,    // ë¶€ëª¨ ë„ë©”ì¸
  subdomain: String,        // ì„œë¸Œë„ë©”ì¸ ë¶€ë¶„ (news, blog, etc.)
  title: String,            // ì„œë¸Œë„ë©”ì¸ ì œëª©
  description: String,      // ì„œë¸Œë„ë©”ì¸ ì„¤ëª…
  type: String,             // "subdomain", "external", "api"
  is_crawlable: Boolean,    // í¬ë¡¤ë§ ê°€ëŠ¥ ì—¬ë¶€
  created_at: DateTime,
  updated_at: DateTime,
  crawl_status: String,
  total_pages: Integer
})
```

#### ğŸ” ì‚¬ìš© ì˜ˆì‹œ
```cypher
// SUBROOT ë…¸ë“œ ìƒì„±
CREATE (subroot:SUBROOT {
  url: "https://news.naver.com",
  parent_domain: "naver.com",
  subdomain: "news",
  title: "ë„¤ì´ë²„ ë‰´ìŠ¤",
  description: "ë‰´ìŠ¤ ì „ë¬¸ ì„œë¹„ìŠ¤",
  type: "subdomain",
  is_crawlable: true,
  created_at: datetime(),
  crawl_status: "completed",
  total_pages: 45
})

// ROOTì™€ SUBROOT ì—°ê²°
MATCH (root:ROOT {domain: "naver.com"})
MATCH (subroot:SUBROOT {parent_domain: "naver.com"})
CREATE (root)-[:HAS_SUBROOT]->(subroot)
```

---

### 3. DEPTH ë…¸ë“œ - í˜ì´ì§€ ê¹Šì´ ê´€ë¦¬

#### ğŸ¯ ëª©ì 
í¬ë¡¤ë§ ê¹Šì´ë³„ í˜ì´ì§€ ê´€ë¦¬ ë° í˜ì´ì§• ì²˜ë¦¬

#### ğŸ“Š ìŠ¤í‚¤ë§ˆ
```cypher
(:DEPTH {
  level: Integer,           // ê¹Šì´ ë ˆë²¨ (1, 2, 3...)
  url: String,              // í˜ì´ì§€ URL
  title: String,            // í˜ì´ì§€ ì œëª©
  description: String,      // í˜ì´ì§€ ì„¤ëª…
  content_preview: String,  // ì½˜í…ì¸  ë¯¸ë¦¬ë³´ê¸° (500ì ì´ë‚´)
  page_type: String,        // "index", "article", "list", "detail"
  
  // í˜ì´ì§• ê´€ë ¨ í•„ë“œ
  is_paginated: Boolean,    // í˜ì´ì§• ì—¬ë¶€
  page_number: Integer,     // í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸
  total_pages: Integer,     // ì´ í˜ì´ì§€ ìˆ˜
  pagination_type: String, // "numbered", "infinite", "load_more"
  
  // ë©”íƒ€ë°ì´í„°
  word_count: Integer,      // ë‹¨ì–´ ìˆ˜
  load_time: Float,         // ë¡œë”© ì‹œê°„ (ì´ˆ)
  status_code: Integer,     // HTTP ìƒíƒœ ì½”ë“œ
  content_type: String,     // MIME íƒ€ì…
  
  // ì‹œê°„ ì •ë³´
  created_at: DateTime,
  updated_at: DateTime,
  last_accessed: DateTime
})
```

#### ğŸ” ì‚¬ìš© ì˜ˆì‹œ
```cypher
// DEPTH ë…¸ë“œ ìƒì„± (ì¼ë°˜ í˜ì´ì§€)
CREATE (depth:DEPTH {
  level: 2,
  url: "https://news.naver.com/sports",
  title: "ìŠ¤í¬ì¸  ë‰´ìŠ¤",
  description: "ìµœì‹  ìŠ¤í¬ì¸  ì†Œì‹",
  content_preview: "ì˜¤ëŠ˜ì˜ ìŠ¤í¬ì¸  ë‰´ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”...",
  page_type: "list",
  is_paginated: true,
  page_number: 1,
  total_pages: 50,
  pagination_type: "numbered",
  word_count: 1500,
  load_time: 2.3,
  status_code: 200,
  content_type: "text/html",
  created_at: datetime(),
  last_accessed: datetime()
})

// í˜ì´ì§• ê´€ê³„ ìƒì„±
MATCH (page1:DEPTH {page_number: 1})
MATCH (page2:DEPTH {page_number: 2})
WHERE page1.url CONTAINS "sports" AND page2.url CONTAINS "sports"
CREATE (page1)-[:NEXT_PAGE]->(page2)
CREATE (page2)-[:PREV_PAGE]->(page1)
```

---

### 4. SECTION ë…¸ë“œ - ë…¼ë¦¬ì  ì„¹ì…˜ (í•µì‹¬!)

#### ğŸ¯ ëª©ì 
ì›¹í˜ì´ì§€ì˜ ì˜ë¯¸ìˆëŠ” êµ¬ì—­ì„ í‘œí˜„í•˜ê³  ìë™ ê·¸ë£¹í•‘

#### ğŸ“Š ìŠ¤í‚¤ë§ˆ
```cypher
(:SECTION {
  id: String,               // UUID
  type: String,             // "header", "nav", "main", "aside", "footer", "paginated_list", "article_list", "search_results"
  title: String,            // ì„¹ì…˜ ì œëª©
  description: String,      // ì„¹ì…˜ ì„¤ëª…
  text: String,             // ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš©
  summary: String,          // AIê°€ ìƒì„±í•œ ìš”ì•½
  
  // ìœ„ì¹˜ ì •ë³´
  xpath: String,            // XPath ê²½ë¡œ
  css_selector: String,     // CSS ì„ íƒì
  order: Integer,           // í˜ì´ì§€ ë‚´ ìˆœì„œ
  
  // ìë™ ê·¸ë£¹í•‘ ì •ë³´
  pattern_type: String,     // "url_pattern", "content_similarity", "dom_structure"
  url_pattern: String,      // URL íŒ¨í„´ (ì˜ˆ: "/news/article-*")
  similarity_score: Float,  // ìœ ì‚¬ë„ ì ìˆ˜ (0.0 - 1.0)
  
  // í˜ì´ì§• ê´€ë ¨ (í˜ì´ì§•ëœ ì„¹ì…˜ì¸ ê²½ìš°)
  is_paginated: Boolean,
  total_pages: Integer,
  sampled_pages: [Integer], // ìƒ˜í”Œë§ëœ í˜ì´ì§€ ë²ˆí˜¸ë“¤
  pagination_url_pattern: String,
  sampling_strategy: String, // "all", "representative", "smart"
  
  // ì½˜í…ì¸  ì •ë³´
  content_count: Integer,   // í•˜ìœ„ ì½˜í…ì¸  ìˆ˜
  content_types: [String],  // í¬í•¨ëœ ì½˜í…ì¸  íƒ€ì…ë“¤
  
  // ì ‘ê·¼ì„± ì •ë³´
  accessibility_level: String, // "easy", "medium", "hard"
  navigation_hint: String,     // íƒìƒ‰ íŒíŠ¸
  keyboard_accessible: Boolean,
  
  // ë©”íƒ€ë°ì´í„°
  importance_score: Float,  // ì¤‘ìš”ë„ ì ìˆ˜
  user_interaction: String, // "high", "medium", "low"
  
  created_at: DateTime,
  updated_at: DateTime
})
```

#### ğŸ” ì‚¬ìš© ì˜ˆì‹œ

##### ì¼ë°˜ ì„¹ì…˜ ìƒì„±
```cypher
// í—¤ë” ì„¹ì…˜
CREATE (header:SECTION {
  id: randomUUID(),
  type: "header",
  title: "ì‚¬ì´íŠ¸ í—¤ë”",
  description: "ë¡œê³ , ê²€ìƒ‰ì°½, ë©”ë‰´ê°€ í¬í•¨ëœ ìƒë‹¨ ì˜ì—­",
  text: "NAVER ê²€ìƒ‰ ë©”ì¼ ì¹´í˜ ë¸”ë¡œê·¸",
  summary: "ë„¤ì´ë²„ ë©”ì¸ ì„œë¹„ìŠ¤ë“¤ì— ëŒ€í•œ ë°”ë¡œê°€ê¸° ë§í¬ ì œê³µ",
  xpath: "//header[@id='header']",
  css_selector: "#header",
  order: 1,
  pattern_type: "dom_structure",
  is_paginated: false,
  content_count: 8,
  content_types: ["logo", "search", "menu"],
  accessibility_level: "easy",
  navigation_hint: "Tab í‚¤ë¡œ ë©”ë‰´ ê°„ ì´ë™ ê°€ëŠ¥",
  keyboard_accessible: true,
  importance_score: 0.9,
  user_interaction: "high",
  created_at: datetime()
})
```

##### í˜ì´ì§•ëœ ì„¹ì…˜ ìƒì„±
```cypher
// í˜ì´ì§•ëœ ë‰´ìŠ¤ ëª©ë¡ ì„¹ì…˜
CREATE (news_list:SECTION {
  id: randomUUID(),
  type: "paginated_list",
  title: "ë‰´ìŠ¤ ëª©ë¡",
  description: "ìµœì‹  ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì˜ ëª©ë¡",
  text: "ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤...",
  summary: "ì´ 100í˜ì´ì§€ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡. ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ, ë¬¸í™” ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ìµœì‹  ì†Œì‹ì„ ì œê³µí•˜ë©°, í•˜ë£¨ í‰ê·  50ê°œì˜ ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.",
  xpath: "//div[@class='news-list']",
  css_selector: ".news-list",
  order: 3,
  pattern_type: "url_pattern",
  url_pattern: "/news/list?page=*",
  is_paginated: true,
  total_pages: 100,
  sampled_pages: [1, 2, 3, 50, 100],
  pagination_url_pattern: "/news/list?page={page}",
  sampling_strategy: "smart",
  content_count: 20,
  content_types: ["article", "title", "summary", "link"],
  accessibility_level: "medium",
  navigation_hint: "ë°©í–¥í‚¤ë¡œ ê¸°ì‚¬ ê°„ ì´ë™, Enterë¡œ ì„ íƒ",
  keyboard_accessible: true,
  importance_score: 0.8,
  user_interaction: "high",
  created_at: datetime()
})
```

---

### 5. ELEMENT ë…¸ë“œ - HTML ìš”ì†Œ

#### ğŸ¯ ëª©ì 
ê°œë³„ HTML ìš”ì†Œì˜ ìƒì„¸ ì •ë³´ ì €ì¥

#### ğŸ“Š ìŠ¤í‚¤ë§ˆ
```cypher
(:ELEMENT {
  id: String,               // UUID
  type: String,             // HTML íƒœê·¸ëª… ("div", "article", "p", "img", etc.)
  class_name: String,       // CSS í´ë˜ìŠ¤ëª…
  element_id: String,       // HTML id ì†ì„±
  text: String,             // ìš”ì†Œì˜ í…ìŠ¤íŠ¸ ë‚´ìš©
  summary: String,          // AI ìš”ì•½
  
  // ìœ„ì¹˜ ì •ë³´
  xpath: String,            // XPath
  css_selector: String,     // CSS ì„ íƒì
  order: Integer,           // ë¶€ëª¨ ë‚´ ìˆœì„œ
  
  // ì†ì„± ì •ë³´
  attributes: Map,          // HTML ì†ì„±ë“¤ (key-value)
  
  // ì½˜í…ì¸  ì •ë³´
  word_count: Integer,      // ë‹¨ì–´ ìˆ˜
  has_children: Boolean,    // í•˜ìœ„ ìš”ì†Œ ì¡´ì¬ ì—¬ë¶€
  children_count: Integer,  // í•˜ìœ„ ìš”ì†Œ ìˆ˜
  
  // ì ‘ê·¼ì„± ì •ë³´
  has_aria_label: Boolean,  // ARIA ë¼ë²¨ ì¡´ì¬ ì—¬ë¶€
  aria_role: String,        // ARIA ì—­í• 
  tab_index: Integer,       // íƒ­ ì¸ë±ìŠ¤
  
  created_at: DateTime,
  updated_at: DateTime
})
```

#### ğŸ” ì‚¬ìš© ì˜ˆì‹œ
```cypher
// ê¸°ì‚¬ ìš”ì†Œ ìƒì„±
CREATE (article:ELEMENT {
  id: randomUUID(),
  type: "article",
  class_name: "news-article",
  element_id: "article-12345",
  text: "AI ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ì¸í•œ ì‚°ì—… ë³€í™”...",
  summary: "AI ê¸°ìˆ ì´ ë‹¤ì–‘í•œ ì‚°ì—…ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•œ ë¶„ì„ ê¸°ì‚¬",
  xpath: "//article[@class='news-article'][1]",
  css_selector: ".news-article:first-child",
  order: 1,
  attributes: {
    "data-article-id": "12345",
    "data-category": "technology"
  },
  word_count: 245,
  has_children: true,
  children_count: 5,
  has_aria_label: true,
  aria_role: "article",
  tab_index: 0,
  created_at: datetime()
})
```

---

### 6. CONTENT ë…¸ë“œ - ìµœì¢… ì½˜í…ì¸ 

#### ğŸ¯ ëª©ì 
ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ì ‘ê·¼í•  ìµœì¢… ì½˜í…ì¸  ì •ë³´

#### ğŸ“Š ìŠ¤í‚¤ë§ˆ
```cypher
(:CONTENT {
  id: String,               // UUID
  type: String,             // "text", "link", "image", "video", "audio", "file"
  text: String,             // ì½˜í…ì¸  í…ìŠ¤íŠ¸
  summary: String,          // AI ìš”ì•½
  
  // ìœ„ì¹˜ ì •ë³´
  xpath: String,
  css_selector: String,
  order: Integer,
  
  // íƒ€ì…ë³„ ì†ì„±
  href: String,             // ë§í¬ì¸ ê²½ìš° URL
  src: String,              // ì´ë¯¸ì§€/ë¹„ë””ì˜¤ì¸ ê²½ìš° ì†ŒìŠ¤ URL
  alt: String,              // ì´ë¯¸ì§€ alt í…ìŠ¤íŠ¸
  title: String,            // ì œëª© ì†ì„±
  
  // ë©”íƒ€ë°ì´í„°
  file_size: Integer,       // íŒŒì¼ í¬ê¸° (ë°”ì´íŠ¸)
  mime_type: String,        // MIME íƒ€ì…
  
  // ë§í¬ ì •ë³´ (typeì´ "link"ì¸ ê²½ìš°)
  link_type: String,        // "internal", "external", "mailto", "tel"
  target_domain: String,    // ëŒ€ìƒ ë„ë©”ì¸
  is_download: Boolean,     // ë‹¤ìš´ë¡œë“œ ë§í¬ ì—¬ë¶€
  
  // ì ‘ê·¼ì„± ì •ë³´
  accessibility_score: Float, // ì ‘ê·¼ì„± ì ìˆ˜
  screen_reader_text: String, // ìŠ¤í¬ë¦° ë¦¬ë”ìš© í…ìŠ¤íŠ¸
  
  // ìƒí˜¸ì‘ìš© ì •ë³´
  is_clickable: Boolean,    // í´ë¦­ ê°€ëŠ¥ ì—¬ë¶€
  requires_javascript: Boolean, // ìë°”ìŠ¤í¬ë¦½íŠ¸ í•„ìš” ì—¬ë¶€
  
  created_at: DateTime,
  updated_at: DateTime
})
```

#### ğŸ” ì‚¬ìš© ì˜ˆì‹œ

##### í…ìŠ¤íŠ¸ ì½˜í…ì¸ 
```cypher
CREATE (text_content:CONTENT {
  id: randomUUID(),
  type: "text",
  text: "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ  ë°œì „ì˜ ìƒˆë¡œìš´ ì „í™˜ì ",
  summary: "AI ê¸°ìˆ  ë°œì „ì— ëŒ€í•œ ì œëª©",
  xpath: "//h2[@class='article-title']",
  css_selector: ".article-title",
  order: 1,
  accessibility_score: 0.9,
  screen_reader_text: "ê¸°ì‚¬ ì œëª©: ì¸ê³µì§€ëŠ¥ ê¸°ìˆ  ë°œì „ì˜ ìƒˆë¡œìš´ ì „í™˜ì ",
  is_clickable: false,
  requires_javascript: false,
  created_at: datetime()
})
```

##### ë§í¬ ì½˜í…ì¸ 
```cypher
CREATE (link_content:CONTENT {
  id: randomUUID(),
  type: "link",
  text: "ì „ì²´ ê¸°ì‚¬ ë³´ê¸°",
  summary: "í•´ë‹¹ ê¸°ì‚¬ì˜ ì „ë¬¸ì„ ë³¼ ìˆ˜ ìˆëŠ” ë§í¬",
  xpath: "//a[@class='read-more']",
  css_selector: ".read-more",
  order: 2,
  href: "https://news.naver.com/article/detail/12345",
  title: "AI ê¸°ìˆ  ë°œì „ ê¸°ì‚¬ ì „ë¬¸",
  link_type: "internal",
  target_domain: "news.naver.com",
  is_download: false,
  accessibility_score: 0.8,
  screen_reader_text: "ë§í¬: ì „ì²´ ê¸°ì‚¬ ë³´ê¸°, ìƒˆ í˜ì´ì§€ì—ì„œ ì—´ë¦¼",
  is_clickable: true,
  requires_javascript: false,
  created_at: datetime()
})
```

---

## ê´€ê³„ êµ¬ì¡° ìƒì„¸ ì„¤ê³„

### ğŸ”— ê¸°ë³¸ ê³„ì¸µ ê´€ê³„

#### 1. HAS_SUBROOT ê´€ê³„
```cypher
(ROOT)-[:HAS_SUBROOT]->(SUBROOT)
```
**ëª©ì **: ë©”ì¸ ë„ë©”ì¸ê³¼ ì„œë¸Œë„ë©”ì¸ ì—°ê²°

**ì†ì„±**:
```cypher
{
  relationship_type: "subdomain",  // "subdomain", "external_link"
  created_at: DateTime,
  is_active: Boolean,
  crawl_priority: Integer          // í¬ë¡¤ë§ ìš°ì„ ìˆœìœ„ (1-10)
}
```

#### 2. HAS_DEPTH ê´€ê³„
```cypher
(ROOT|SUBROOT)-[:HAS_DEPTH]->(DEPTH)
```
**ëª©ì **: ë„ë©”ì¸ê³¼ ê¹Šì´ë³„ í˜ì´ì§€ ì—°ê²°

**ì†ì„±**:
```cypher
{
  depth_level: Integer,            // ê¹Šì´ ë ˆë²¨
  crawl_order: Integer,           // í¬ë¡¤ë§ ìˆœì„œ
  parent_url: String,             // ë¶€ëª¨ í˜ì´ì§€ URL
  created_at: DateTime
}
```

#### 3. HAS_SECTION ê´€ê³„
```cypher
(DEPTH)-[:HAS_SECTION]->(SECTION)
```
**ëª©ì **: í˜ì´ì§€ì™€ ì„¹ì…˜ ì—°ê²°

**ì†ì„±**:
```cypher
{
  section_position: String,        // "top", "middle", "bottom"
  visual_order: Integer,          // ì‹œê°ì  ìˆœì„œ
  extraction_confidence: Float,   // ì¶”ì¶œ ì‹ ë¢°ë„
  created_at: DateTime
}
```

#### 4. HAS_ELEMENT ê´€ê³„
```cypher
(SECTION)-[:HAS_ELEMENT]->(ELEMENT)
```
**ëª©ì **: ì„¹ì…˜ê³¼ ìš”ì†Œ ì—°ê²°

**ì†ì„±**:
```cypher
{
  element_importance: Float,       // ìš”ì†Œ ì¤‘ìš”ë„
  dom_depth: Integer,             // DOM íŠ¸ë¦¬ ê¹Šì´
  created_at: DateTime
}
```

#### 5. HAS_CONTENT ê´€ê³„
```cypher
(ELEMENT)-[:HAS_CONTENT]->(CONTENT)
```
**ëª©ì **: ìš”ì†Œì™€ ì½˜í…ì¸  ì—°ê²°

**ì†ì„±**:
```cypher
{
  content_priority: Integer,       // ì½˜í…ì¸  ìš°ì„ ìˆœìœ„
  user_interaction_score: Float,  // ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ì ìˆ˜
  created_at: DateTime
}
```

### ğŸ”€ íƒìƒ‰ ê´€ê³„

#### 1. LINKS_TO ê´€ê³„
```cypher
(CONTENT)-[:LINKS_TO]->(ROOT|SUBROOT|DEPTH)
```
**ëª©ì **: ì½˜í…ì¸  ê°„ ë§í¬ ì—°ê²°

**ì†ì„±**:
```cypher
{
  link_type: String,              // "internal", "external", "anchor"
  link_text: String,              // ë§í¬ í…ìŠ¤íŠ¸
  confidence: Float,              // ë§í¬ ìœ íš¨ì„± ì‹ ë¢°ë„
  last_checked: DateTime,         // ë§ˆì§€ë§‰ í™•ì¸ ì‹œê°„
  status: String,                 // "active", "broken", "redirected"
  http_status: Integer,           // HTTP ìƒíƒœ ì½”ë“œ
  created_at: DateTime
}
```

#### 2. NEXT_DEPTH ê´€ê³„
```cypher
(DEPTH)-[:NEXT_DEPTH]->(DEPTH)
```
**ëª©ì **: ê¹Šì´ë³„ í˜ì´ì§€ ìˆœì„œ ê´€ë¦¬

**ì†ì„±**:
```cypher
{
  sequence_number: Integer,       // ìˆœì„œ ë²ˆí˜¸
  navigation_hint: String,        // íƒìƒ‰ íŒíŠ¸
  created_at: DateTime
}
```

#### 3. NEXT_PAGE / PREV_PAGE ê´€ê³„
```cypher
(DEPTH)-[:NEXT_PAGE]->(DEPTH)
(DEPTH)-[:PREV_PAGE]->(DEPTH)
```
**ëª©ì **: í˜ì´ì§• ê´€ê³„ ê´€ë¦¬

**ì†ì„±**:
```cypher
{
  page_diff: Integer,             // í˜ì´ì§€ ì°¨ì´
  pagination_type: String,        // "sequential", "jump"
  created_at: DateTime
}
```

### ğŸ¯ íŠ¹ìˆ˜ ê´€ê³„

#### 1. SIMILAR_TO ê´€ê³„
```cypher
(SECTION)-[:SIMILAR_TO]->(SECTION)
```
**ëª©ì **: ìœ ì‚¬í•œ ì„¹ì…˜ ê°„ ì—°ê²°

**ì†ì„±**:
```cypher
{
  similarity_score: Float,        // ìœ ì‚¬ë„ ì ìˆ˜ (0.0 - 1.0)
  similarity_type: String,        // "content", "structure", "function"
  algorithm_used: String,         // ì‚¬ìš©ëœ ìœ ì‚¬ë„ ì•Œê³ ë¦¬ì¦˜
  created_at: DateTime
}
```

#### 2. GROUPED_BY ê´€ê³„
```cypher
(DEPTH)-[:GROUPED_BY]->(SECTION)
```
**ëª©ì **: ìë™ ê·¸ë£¹í•‘ëœ í˜ì´ì§€ì™€ ì„¹ì…˜ ì—°ê²°

**ì†ì„±**:
```cypher
{
  group_confidence: Float,        // ê·¸ë£¹í•‘ ì‹ ë¢°ë„
  grouping_criteria: String,      // "url_pattern", "content_type", "structure"
  auto_generated: Boolean,        // ìë™ ìƒì„± ì—¬ë¶€
  created_at: DateTime
}
```

---

## ì¸ë±ìŠ¤ ë° ì œì•½ ì¡°ê±´

### ğŸ”’ ì œì•½ ì¡°ê±´ (Constraints)

#### 1. ìœ ë‹ˆí¬ ì œì•½ ì¡°ê±´
```cypher
-- ROOT ë…¸ë“œ ë„ë©”ì¸ ìœ ë‹ˆí¬
CREATE CONSTRAINT root_domain_unique IF NOT EXISTS
FOR (r:ROOT) REQUIRE r.domain IS UNIQUE

-- SUBROOT ë…¸ë“œ URL ìœ ë‹ˆí¬
CREATE CONSTRAINT subroot_url_unique IF NOT EXISTS
FOR (s:SUBROOT) REQUIRE s.url IS UNIQUE

-- DEPTH ë…¸ë“œ URL ìœ ë‹ˆí¬
CREATE CONSTRAINT depth_url_unique IF NOT EXISTS
FOR (d:DEPTH) REQUIRE d.url IS UNIQUE

-- SECTION ë…¸ë“œ ID ìœ ë‹ˆí¬
CREATE CONSTRAINT section_id_unique IF NOT EXISTS
FOR (s:SECTION) REQUIRE s.id IS UNIQUE

-- ELEMENT ë…¸ë“œ ID ìœ ë‹ˆí¬
CREATE CONSTRAINT element_id_unique IF NOT EXISTS
FOR (e:ELEMENT) REQUIRE e.id IS UNIQUE

-- CONTENT ë…¸ë“œ ID ìœ ë‹ˆí¬
CREATE CONSTRAINT content_id_unique IF NOT EXISTS
FOR (c:CONTENT) REQUIRE c.id IS UNIQUE
```

#### 2. ì¡´ì¬ ì œì•½ ì¡°ê±´
```cypher
-- ROOT ë…¸ë“œ í•„ìˆ˜ í•„ë“œ
CREATE CONSTRAINT root_url_exists IF NOT EXISTS
FOR (r:ROOT) REQUIRE r.url IS NOT NULL

CREATE CONSTRAINT root_domain_exists IF NOT EXISTS
FOR (r:ROOT) REQUIRE r.domain IS NOT NULL

-- SECTION ë…¸ë“œ í•„ìˆ˜ í•„ë“œ
CREATE CONSTRAINT section_type_exists IF NOT EXISTS
FOR (s:SECTION) REQUIRE s.type IS NOT NULL

-- CONTENT ë…¸ë“œ í•„ìˆ˜ í•„ë“œ
CREATE CONSTRAINT content_type_exists IF NOT EXISTS
FOR (c:CONTENT) REQUIRE c.type IS NOT NULL
```

### ğŸ“Š ì¸ë±ìŠ¤ (Indexes)

#### 1. ê¸°ë³¸ ì¸ë±ìŠ¤
```cypher
-- ìì£¼ ê²€ìƒ‰ë˜ëŠ” í•„ë“œë“¤
CREATE INDEX root_url_index IF NOT EXISTS
FOR (r:ROOT) ON (r.url)

CREATE INDEX root_domain_index IF NOT EXISTS
FOR (r:ROOT) ON (r.domain)

CREATE INDEX depth_level_index IF NOT EXISTS
FOR (d:DEPTH) ON (d.level)

CREATE INDEX section_type_index IF NOT EXISTS
FOR (s:SECTION) ON (s.type)

CREATE INDEX content_type_index IF NOT EXISTS
FOR (c:CONTENT) ON (c.type)
```

#### 2. ë³µí•© ì¸ë±ìŠ¤
```cypher
-- í˜ì´ì§• ê´€ë ¨
CREATE INDEX depth_pagination_index IF NOT EXISTS
FOR (d:DEPTH) ON (d.page_number, d.total_pages)

-- ì„¹ì…˜ ì¤‘ìš”ë„ ê´€ë ¨
CREATE INDEX section_importance_index IF NOT EXISTS
FOR (s:SECTION) ON (s.importance_score, s.type)

-- ì‹œê°„ ê¸°ë°˜ ì¸ë±ìŠ¤
CREATE INDEX created_at_index IF NOT EXISTS
FOR (n) ON (n.created_at) WHERE n.created_at IS NOT NULL
```

#### 3. í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤
```cypher
-- ì „ë¬¸ ê²€ìƒ‰ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤
CREATE FULLTEXT INDEX section_text_index IF NOT EXISTS
FOR (s:SECTION) ON EACH [s.title, s.description, s.text, s.summary]

CREATE FULLTEXT INDEX content_text_index IF NOT EXISTS
FOR (c:CONTENT) ON EACH [c.text, c.summary]
```

---

## Section ìë™ ìƒì„± ì‹œìŠ¤í…œ

### ğŸ§  ìë™ ìƒì„± ì•Œê³ ë¦¬ì¦˜

#### 1. íŒ¨í„´ ê°ì§€ ì‹œìŠ¤í…œ

##### URL íŒ¨í„´ ë¶„ì„
```python
class URLPatternAnalyzer:
    def __init__(self):
        self.patterns = {
            'article': r'/article/(\d+)',
            'news': r'/news/(\d+)',
            'post': r'/post/(\d+)',
            'page': r'/page/(\d+)',
            'category': r'/category/([^/]+)',
            'tag': r'/tag/([^/]+)',
            'date': r'/(\d{4})/(\d{2})/(\d{2})',
            'user': r'/user/([^/]+)',
            'product': r'/product/([^/]+)'
        }
    
    def analyze_urls(self, urls: List[str]) -> Dict[str, List[str]]:
        """URL íŒ¨í„´ ë¶„ì„"""
        grouped_urls = {}
        for url in urls:
            pattern = self.identify_pattern(url)
            if pattern:
                if pattern not in grouped_urls:
                    grouped_urls[pattern] = []
                grouped_urls[pattern].append(url)
        return grouped_urls
    
    def identify_pattern(self, url: str) -> Optional[str]:
        """ê°œë³„ URLì˜ íŒ¨í„´ ì‹ë³„"""
        for pattern_name, regex in self.patterns.items():
            if re.search(regex, url):
                return pattern_name
        return None
```

##### êµ¬ì¡° ìœ ì‚¬ë„ ë¶„ì„
```python
class StructureSimilarityAnalyzer:
    def __init__(self):
        self.similarity_threshold = 0.85
    
    def analyze_structure(self, pages: List[Dict]) -> Dict:
        """í˜ì´ì§€ êµ¬ì¡° ìœ ì‚¬ë„ ë¶„ì„"""
        similarity_matrix = self.calculate_similarity_matrix(pages)
        clusters = self.cluster_similar_pages(similarity_matrix)
        return self.create_section_candidates(clusters)
    
    def calculate_similarity_matrix(self, pages: List[Dict]) -> np.ndarray:
        """í˜ì´ì§€ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
        # DOM êµ¬ì¡° ë¹„êµ
        # ì½˜í…ì¸  íƒ€ì… ë¹„êµ
        # ë ˆì´ì•„ì›ƒ íŒ¨í„´ ë¹„êµ
        pass
    
    def cluster_similar_pages(self, similarity_matrix: np.ndarray) -> List[List[int]]:
        """ìœ ì‚¬í•œ í˜ì´ì§€ë“¤ì„ í´ëŸ¬ìŠ¤í„°ë§"""
        # ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ë˜ëŠ” DBSCAN ì‚¬ìš©
        pass
```

#### 2. Section ìƒì„± ì¡°ê±´

##### ì¡°ê±´ 1: ì„ê³„ê°’ í™•ì¸
```cypher
// ìœ ì‚¬í•œ í˜ì´ì§€ ìˆ˜ í™•ì¸
MATCH (d:DEPTH)
WHERE d.url CONTAINS '/news/'
WITH count(d) as page_count
WHERE page_count >= 3
RETURN page_count
```

##### ì¡°ê±´ 2: êµ¬ì¡° ìœ ì‚¬ë„ ê²€ì¦
```cypher
// êµ¬ì¡° ìœ ì‚¬ë„ ê¸°ë°˜ ê·¸ë£¹í•‘
MATCH (d1:DEPTH), (d2:DEPTH)
WHERE d1.url CONTAINS '/news/' AND d2.url CONTAINS '/news/'
  AND d1.structure_hash = d2.structure_hash
WITH count(DISTINCT d1) as similar_pages
WHERE similar_pages >= 3
RETURN similar_pages
```

##### ì¡°ê±´ 3: ì½˜í…ì¸  íƒ€ì… ì¼ì¹˜
```cypher
// ì½˜í…ì¸  íƒ€ì… ê¸°ë°˜ ê·¸ë£¹í•‘
MATCH (d:DEPTH)
WHERE d.page_type = 'article'
WITH d.content_category as category, count(d) as page_count
WHERE page_count >= 3
RETURN category, page_count
```

#### 3. Section ìë™ ìƒì„± í”„ë¡œì„¸ìŠ¤

##### ë‹¨ê³„ 1: íŒ¨í„´ ê°ì§€
```cypher
// 1. URL íŒ¨í„´ ë¶„ì„
MATCH (d:DEPTH)
WHERE d.url =~ '.*/news/article-\\d+.*'
WITH d
ORDER BY d.url
LIMIT 100

// 2. íŒ¨í„´ ê²€ì¦
WITH collect(d) as articles
WHERE size(articles) >= 3

// 3. Section ìƒì„±
CREATE (s:SECTION {
  id: randomUUID(),
  type: "article_list",
  title: "ë‰´ìŠ¤ ê¸°ì‚¬ ì„¹ì…˜",
  description: "ë‰´ìŠ¤ ê¸°ì‚¬ " + size(articles) + "ê°œ í¬í•¨",
  pattern_type: "url_pattern",
  url_pattern: "/news/article-*",
  content_count: size(articles),
  is_paginated: false,
  auto_generated: true,
  created_at: datetime()
})

// 4. ê´€ê³„ ìƒì„±
WITH s, articles
UNWIND articles as article
CREATE (article)-[:GROUPED_BY]->(s)
```

##### ë‹¨ê³„ 2: ìƒ˜í”Œë§ ì „ëµ
```cypher
// ëŒ€í‘œ í˜ì´ì§€ ì„ íƒ
MATCH (d:DEPTH)-[:GROUPED_BY]->(s:SECTION)
WHERE s.id = $section_id
WITH d, s
ORDER BY d.importance_score DESC, d.created_at DESC
LIMIT 5

// ìƒ˜í”Œ í˜ì´ì§€ ì„¤ì •
SET s.sample_pages = collect(d.url)
```

##### ë‹¨ê³„ 3: ìš”ì•½ ìƒì„±
```python
async def generate_section_summary(section_id: str, sample_pages: List[str]) -> str:
    """Section ìš”ì•½ ìƒì„±"""
    # ìƒ˜í”Œ í˜ì´ì§€ë“¤ì˜ ë‚´ìš© ë¶„ì„
    content_analysis = await analyze_sample_contents(sample_pages)
    
    # LangChainì„ í†µí•œ ìš”ì•½ ìƒì„±
    prompt = f"""
    ë‹¤ìŒì€ {len(sample_pages)}ê°œì˜ ì›¹í˜ì´ì§€ ìƒ˜í”Œ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:
    
    ê³µí†µ ì£¼ì œ: {content_analysis['common_topics']}
    ì½˜í…ì¸  íƒ€ì…: {content_analysis['content_types']}
    ì—…ë°ì´íŠ¸ ë¹ˆë„: {content_analysis['update_frequency']}
    ì‚¬ìš©ì ê´€ì‹¬ë„: {content_analysis['user_interest']}
    
    ì´ ì„¹ì…˜ì˜ íŠ¹ì§•ê³¼ í¬í•¨ëœ ì½˜í…ì¸ ë¥¼ 100ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
    """
    
    summary = await langchain_service.generate_summary(prompt)
    
    # Section ë…¸ë“œ ì—…ë°ì´íŠ¸
    query = """
    MATCH (s:SECTION {id: $section_id})
    SET s.summary = $summary,
        s.updated_at = datetime()
    """
    
    await neo4j_service.run_query(query, {
        'section_id': section_id,
        'summary': summary
    })
    
    return summary
```

---

## í˜ì´ì§• ì²˜ë¦¬ ì•„í‚¤í…ì²˜

### ğŸ”„ í˜ì´ì§• ê°ì§€ ì‹œìŠ¤í…œ

#### 1. ê°ì§€ ë°©ë²•ë“¤

##### DOM ê¸°ë°˜ ê°ì§€
```python
class DOMPaginationDetector:
    def __init__(self):
        self.pagination_selectors = [
            '.pagination',
            '.paging',
            '.page-navigation',
            'nav[aria-label*="page"]',
            '[class*="page-number"]',
            '.next-page',
            '.prev-page',
            '[rel="next"]',
            '[rel="prev"]',
            '.page-link'
        ]
    
    def detect_pagination(self, soup: BeautifulSoup) -> Optional[Dict]:
        """DOM ê¸°ë°˜ í˜ì´ì§• ê°ì§€"""
        for selector in self.pagination_selectors:
            elements = soup.select(selector)
            if elements:
                return self.extract_pagination_info(elements[0])
        return None
    
    def extract_pagination_info(self, element) -> Dict:
        """í˜ì´ì§• ì •ë³´ ì¶”ì¶œ"""
        info = {
            'type': 'dom_based',
            'total_pages': self.extract_total_pages(element),
            'current_page': self.extract_current_page(element),
            'next_page_url': self.extract_next_page_url(element),
            'prev_page_url': self.extract_prev_page_url(element)
        }
        return info
```

##### URL íŒ¨í„´ ê°ì§€
```python
class URLPatternDetector:
    def __init__(self):
        self.url_patterns = [
            r'[?&]page=(\d+)',
            r'[?&]p=(\d+)',
            r'/page/(\d+)',
            r'[?&]offset=(\d+)',
            r'[?&]start=(\d+)',
            r'[?&]pageNum=(\d+)',
            r'[?&]pageNumber=(\d+)'
        ]
    
    def detect_pagination(self, current_url: str, links: List[str]) -> Optional[Dict]:
        """URL íŒ¨í„´ ê¸°ë°˜ í˜ì´ì§• ê°ì§€"""
        for pattern in self.url_patterns:
            if self.analyze_pattern(current_url, links, pattern):
                return self.extract_pattern_info(current_url, links, pattern)
        return None
    
    def analyze_pattern(self, current_url: str, links: List[str], pattern: str) -> bool:
        """íŒ¨í„´ ë¶„ì„"""
        current_match = re.search(pattern, current_url)
        if not current_match:
            return False
        
        # ë‹¤ë¥¸ í˜ì´ì§€ ë§í¬ë“¤ì—ì„œ ê°™ì€ íŒ¨í„´ í™•ì¸
        pattern_matches = 0
        for link in links:
            if re.search(pattern, link):
                pattern_matches += 1
        
        return pattern_matches >= 2  # ìµœì†Œ 2ê°œ ì´ìƒì˜ íŒ¨í„´ ë§¤ì¹˜
```

#### 2. í˜ì´ì§• ì •ë³´ ì €ì¥

##### í˜ì´ì§• ë©”íƒ€ë°ì´í„° ì €ì¥
```cypher
// í˜ì´ì§•ëœ SECTION ìƒì„±
CREATE (s:SECTION {
  id: randomUUID(),
  type: "paginated_list",
  title: "ë‰´ìŠ¤ ëª©ë¡",
  description: "ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ (í˜ì´ì§•ë¨)",
  is_paginated: true,
  
  // í˜ì´ì§• ì •ë³´
  total_pages: 100,
  current_page: 1,
  pages_per_section: 20,
  pagination_type: "numbered",
  
  // URL íŒ¨í„´
  base_url: "https://news.example.com/list",
  pagination_url_pattern: "https://news.example.com/list?page={page}",
  
  // ìƒ˜í”Œë§ ì •ë³´
  sampling_strategy: "smart",
  sampled_pages: [1, 2, 3, 25, 50, 75, 100],
  
  created_at: datetime()
})

// ê° í˜ì´ì§€ë¥¼ DEPTH ë…¸ë“œë¡œ ìƒì„±
UNWIND [1, 2, 3, 25, 50, 75, 100] as page_num
CREATE (d:DEPTH {
  level: 1,
  url: "https://news.example.com/list?page=" + toString(page_num),
  title: "ë‰´ìŠ¤ ëª©ë¡ - " + toString(page_num) + "í˜ì´ì§€",
  page_number: page_num,
  is_paginated: true,
  parent_section_id: s.id,
  created_at: datetime()
})
CREATE (s)-[:HAS_DEPTH]->(d)
```

##### í˜ì´ì§€ ê°„ ê´€ê³„ ì„¤ì •
```cypher
// ìˆœì°¨ì  í˜ì´ì§€ ê´€ê³„
MATCH (d1:DEPTH {page_number: 1})-[:HAS_DEPTH]-(s:SECTION),
      (d2:DEPTH {page_number: 2})-[:HAS_DEPTH]-(s)
CREATE (d1)-[:NEXT_PAGE {page_diff: 1}]->(d2)
CREATE (d2)-[:PREV_PAGE {page_diff: 1}]->(d1)

// ì í”„ í˜ì´ì§€ ê´€ê³„
MATCH (d1:DEPTH {page_number: 1})-[:HAS_DEPTH]-(s:SECTION),
      (d100:DEPTH {page_number: 100})-[:HAS_DEPTH]-(s)
CREATE (d1)-[:NEXT_PAGE {page_diff: 99, pagination_type: "jump"}]->(d100)
```

### ğŸ¯ ì ì‘í˜• ìƒ˜í”Œë§ ì „ëµ

#### 1. ìƒ˜í”Œë§ ì „ëµ ê²°ì •
```python
class SamplingStrategyDecider:
    def __init__(self):
        self.strategies = {
            'all': self.sample_all_pages,
            'representative': self.sample_representative_pages,
            'smart': self.sample_smart_pages,
            'first_n': self.sample_first_n_pages,
            'boundary': self.sample_boundary_pages
        }
    
    def determine_strategy(self, total_pages: int, content_type: str, 
                         user_priority: str = 'balanced') -> str:
        """ìƒ˜í”Œë§ ì „ëµ ê²°ì •"""
        if total_pages <= 5:
            return 'all'
        elif content_type in ['news', 'feed', 'updates'] and user_priority == 'latest':
            return 'first_n'
        elif total_pages <= 20:
            return 'representative'
        elif content_type in ['archive', 'search_results']:
            return 'boundary'
        else:
            return 'smart'
    
    def sample_all_pages(self, total_pages: int) -> List[int]:
        """ëª¨ë“  í˜ì´ì§€ ìƒ˜í”Œë§"""
        return list(range(1, total_pages + 1))
    
    def sample_representative_pages(self, total_pages: int) -> List[int]:
        """ëŒ€í‘œ í˜ì´ì§€ ìƒ˜í”Œë§"""
        if total_pages <= 10:
            return [1, 2, 3, total_pages]
        else:
            mid = total_pages // 2
            return [1, 2, 3, mid, total_pages]
    
    def sample_smart_pages(self, total_pages: int) -> List[int]:
        """ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§"""
        if total_pages <= 50:
            return [1, 2, 3, 10, 20, total_pages]
        else:
            return [1, 2, 3, 10, 25, 50, total_pages // 2, total_pages]
    
    def sample_first_n_pages(self, total_pages: int, n: int = 5) -> List[int]:
        """ì²˜ìŒ Nê°œ í˜ì´ì§€ ìƒ˜í”Œë§"""
        return list(range(1, min(n + 1, total_pages + 1)))
    
    def sample_boundary_pages(self, total_pages: int) -> List[int]:
        """ê²½ê³„ í˜ì´ì§€ ìƒ˜í”Œë§"""
        if total_pages <= 10:
            return [1, 2, total_pages - 1, total_pages]
        else:
            return [1, 2, 3, total_pages - 2, total_pages - 1, total_pages]
```

#### 2. ìƒ˜í”Œë§ ì‹¤í–‰ ë° ì €ì¥
```python
async def execute_sampling_strategy(section_id: str, strategy: str, 
                                   total_pages: int, base_url: str):
    """ìƒ˜í”Œë§ ì „ëµ ì‹¤í–‰"""
    decider = SamplingStrategyDecider()
    sampled_pages = decider.strategies[strategy](total_pages)
    
    # ìƒ˜í”Œ í˜ì´ì§€ í¬ë¡¤ë§
    for page_num in sampled_pages:
        page_url = base_url.replace('{page}', str(page_num))
        
        # í˜ì´ì§€ í¬ë¡¤ë§
        page_content = await crawl_page(page_url)
        
        # DEPTH ë…¸ë“œ ìƒì„±
        query = """
        MATCH (s:SECTION {id: $section_id})
        CREATE (d:DEPTH {
          level: 1,
          url: $page_url,
          title: $page_title,
          page_number: $page_num,
          is_paginated: true,
          content_preview: $content_preview,
          word_count: $word_count,
          created_at: datetime()
        })
        CREATE (s)-[:HAS_DEPTH {sampling_strategy: $strategy}]->(d)
        """
        
        await neo4j_service.run_query(query, {
            'section_id': section_id,
            'page_url': page_url,
            'page_title': page_content['title'],
            'page_num': page_num,
            'content_preview': page_content['preview'][:500],
            'word_count': page_content['word_count'],
            'strategy': strategy
        })
```

---

## ë°ì´í„° ì €ì¥ ìµœì í™”

### ğŸš€ ì„±ëŠ¥ ìµœì í™” ì „ëµ

#### 1. ë°°ì¹˜ ì²˜ë¦¬
```python
class BatchProcessor:
    def __init__(self, batch_size: int = 1000):
        self.batch_size = batch_size
        self.pending_nodes = []
        self.pending_relationships = []
    
    async def add_node(self, node_data: Dict):
        """ë…¸ë“œ ë°°ì¹˜ì— ì¶”ê°€"""
        self.pending_nodes.append(node_data)
        if len(self.pending_nodes) >= self.batch_size:
            await self.flush_nodes()
    
    async def add_relationship(self, rel_data: Dict):
        """ê´€ê³„ ë°°ì¹˜ì— ì¶”ê°€"""
        self.pending_relationships.append(rel_data)
        if len(self.pending_relationships) >= self.batch_size:
            await self.flush_relationships()
    
    async def flush_nodes(self):
        """ë…¸ë“œ ë°°ì¹˜ ì²˜ë¦¬"""
        if not self.pending_nodes:
            return
        
        # ë…¸ë“œ íƒ€ì…ë³„ ë¶„ë¥˜
        nodes_by_type = {}
        for node in self.pending_nodes:
            node_type = node['type']
            if node_type not in nodes_by_type:
                nodes_by_type[node_type] = []
            nodes_by_type[node_type].append(node)
        
        # íƒ€ì…ë³„ ë°°ì¹˜ ìƒì„±
        for node_type, nodes in nodes_by_type.items():
            query = f"""
            UNWIND $nodes as node
            CREATE (n:{node_type})
            SET n = node.properties
            """
            await neo4j_service.run_query(query, {'nodes': nodes})
        
        self.pending_nodes.clear()
    
    async def flush_relationships(self):
        """ê´€ê³„ ë°°ì¹˜ ì²˜ë¦¬"""
        if not self.pending_relationships:
            return
        
        # ê´€ê³„ íƒ€ì…ë³„ ì²˜ë¦¬
        rels_by_type = {}
        for rel in self.pending_relationships:
            rel_type = rel['type']
            if rel_type not in rels_by_type:
                rels_by_type[rel_type] = []
            rels_by_type[rel_type].append(rel)
        
        for rel_type, rels in rels_by_type.items():
            query = f"""
            UNWIND $rels as rel
            MATCH (from {{id: rel.from_id}})
            MATCH (to {{id: rel.to_id}})
            CREATE (from)-[r:{rel_type}]->(to)
            SET r = rel.properties
            """
            await neo4j_service.run_query(query, {'rels': rels})
        
        self.pending_relationships.clear()
```

#### 2. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
```python
class MemoryOptimizedStorage:
    def __init__(self, max_memory_mb: int = 512):
        self.max_memory_mb = max_memory_mb
        self.current_memory_usage = 0
        self.text_compression = True
        self.image_optimization = True
    
    def optimize_text_content(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì½˜í…ì¸  ìµœì í™”"""
        if not text:
            return text
        
        # ì¤‘ë³µ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        text = re.sub(r'[^\w\s\-.,!?()]', '', text)
        
        # ê¸¸ì´ ì œí•œ
        if len(text) > 5000:
            text = text[:5000] + "..."
        
        return text.strip()
    
    def calculate_storage_size(self, data: Dict) -> int:
        """ë°ì´í„° ì €ì¥ í¬ê¸° ê³„ì‚°"""
        import sys
        return sys.getsizeof(str(data))
    
    async def store_with_memory_check(self, data: Dict):
        """ë©”ëª¨ë¦¬ ì²´í¬ í›„ ì €ì¥"""
        data_size = self.calculate_storage_size(data)
        
        if self.current_memory_usage + data_size > self.max_memory_mb * 1024 * 1024:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            await self.cleanup_memory()
        
        # ë°ì´í„° ìµœì í™”
        optimized_data = self.optimize_data(data)
        
        # ì €ì¥
        await self.store_data(optimized_data)
        
        self.current_memory_usage += data_size
    
    def optimize_data(self, data: Dict) -> Dict:
        """ë°ì´í„° ìµœì í™”"""
        optimized = {}
        
        for key, value in data.items():
            if isinstance(value, str):
                optimized[key] = self.optimize_text_content(value)
            elif isinstance(value, list) and len(value) > 100:
                # í° ë¦¬ìŠ¤íŠ¸ëŠ” ìƒ˜í”Œë§
                optimized[key] = value[:50] + value[-50:]
            else:
                optimized[key] = value
        
        return optimized
```

#### 3. ì¤‘ë³µ ì œê±°
```cypher
-- ì¤‘ë³µ ë…¸ë“œ ë°©ì§€ë¥¼ ìœ„í•œ MERGE ì‚¬ìš©
MERGE (r:ROOT {domain: $domain})
ON CREATE SET 
  r.url = $url,
  r.title = $title,
  r.created_at = datetime()
ON MATCH SET 
  r.updated_at = datetime(),
  r.last_crawled = datetime()

-- ì¤‘ë³µ ê´€ê³„ ë°©ì§€
MATCH (from {id: $from_id}), (to {id: $to_id})
MERGE (from)-[r:LINKS_TO]->(to)
ON CREATE SET 
  r.created_at = datetime(),
  r.link_text = $link_text
ON MATCH SET 
  r.last_checked = datetime(),
  r.status = $status
```

---

## ì§€ëŠ¥í˜• ê²€ìƒ‰ ì‹œìŠ¤í…œ (textToCypher & Embedding)

### ğŸ¤– textToCypher ì‹œìŠ¤í…œ

#### 1. ìì—°ì–´ ì§ˆì˜ ì²˜ë¦¬
```python
class TextToCypherProcessor:
    def __init__(self, langchain_service):
        self.langchain = langchain_service
        self.query_templates = {
            'find_content': "MATCH (c:CONTENT) WHERE c.text CONTAINS $search_term RETURN c",
            'find_section': "MATCH (s:SECTION) WHERE s.title CONTAINS $search_term RETURN s",
            'navigation_path': "MATCH path = (root:ROOT)-[:HAS_DEPTH*1..3]->(depth:DEPTH) WHERE root.domain = $domain RETURN path"
        }
    
    async def process_natural_query(self, query: str, domain: str) -> str:
        """ìì—°ì–´ ì§ˆì˜ë¥¼ Cypher ì¿¼ë¦¬ë¡œ ë³€í™˜"""
        prompt = f"""
        ë‹¤ìŒ ìì—°ì–´ ì§ˆì˜ë¥¼ Neo4j Cypher ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ì„¸ìš”:
        
        ì§ˆì˜: {query}
        ëŒ€ìƒ ë„ë©”ì¸: {domain}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ ë…¸ë“œ íƒ€ì…:
        - ROOT: ì›¹ì‚¬ì´íŠ¸ ë£¨íŠ¸ (domain, title, description)
        - SECTION: í˜ì´ì§€ ì„¹ì…˜ (title, summary, type, is_paginated)
        - CONTENT: ìµœì¢… ì½˜í…ì¸  (text, type, href)
        
        ê´€ê³„:
        - HAS_DEPTH, HAS_SECTION, HAS_CONTENT, LINKS_TO
        """
        
        cypher_query = await self.langchain.generate_cypher(prompt)
        return self.validate_and_sanitize_query(cypher_query)
    
    def validate_and_sanitize_query(self, query: str) -> str:
        """ì¿¼ë¦¬ ê²€ì¦ ë° ì •ë¦¬"""
        # ë³´ì•ˆ ê²€ì¦
        dangerous_keywords = ['DROP', 'DELETE', 'DETACH', 'REMOVE', 'SET', 'CREATE']
        for keyword in dangerous_keywords:
            if keyword in query.upper():
                raise ValueError(f"Dangerous keyword '{keyword}' detected")
        
        # ì¿¼ë¦¬ ì •ë¦¬
        query = query.strip()
        if not query.upper().startswith('MATCH'):
            raise ValueError("Query must start with MATCH")
        
        return query
```

#### 2. ì¿¼ë¦¬ ì‹¤í–‰ ë° ê²°ê³¼ ì²˜ë¦¬
```python
class QueryExecutor:
    def __init__(self, neo4j_service, text_to_cypher):
        self.neo4j = neo4j_service
        self.text_to_cypher = text_to_cypher
    
    async def execute_natural_query(self, natural_query: str, domain: str) -> dict:
        """ìì—°ì–´ ì§ˆì˜ ì‹¤í–‰"""
        try:
            # 1. ìì—°ì–´ë¥¼ Cypherë¡œ ë³€í™˜
            cypher_query = await self.text_to_cypher.process_natural_query(
                natural_query, domain
            )
            
            # 2. ì¿¼ë¦¬ ì‹¤í–‰
            results = await self.neo4j.run_query(cypher_query, {'domain': domain})
            
            # 3. ê²°ê³¼ í›„ì²˜ë¦¬
            formatted_results = self.format_results(results)
            
            return {
                'query': natural_query,
                'cypher': cypher_query,
                'results': formatted_results,
                'status': 'success'
            }
            
        except Exception as e:
            return {
                'query': natural_query,
                'error': str(e),
                'status': 'error'
            }
    
    def format_results(self, raw_results: list) -> list:
        """ê²°ê³¼ í¬ë§·íŒ…"""
        formatted = []
        for result in raw_results:
            if 'c' in result:  # CONTENT ë…¸ë“œ
                formatted.append({
                    'type': 'content',
                    'text': result['c']['text'],
                    'content_type': result['c']['type']
                })
            elif 's' in result:  # SECTION ë…¸ë“œ
                formatted.append({
                    'type': 'section',
                    'title': result['s']['title'],
                    'summary': result['s']['summary']
                })
        return formatted
```

### ğŸ” Embedding ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰

#### 1. ë…¸ë“œ ì„ë² ë”© í•„ë“œ ì¶”ê°€
```cypher
-- SECTION ë…¸ë“œì— ì„ë² ë”© í•„ë“œ ì¶”ê°€
ALTER TABLE SECTION ADD COLUMN embedding VECTOR(1536);

-- CONTENT ë…¸ë“œì— ì„ë² ë”© í•„ë“œ ì¶”ê°€  
ALTER TABLE CONTENT ADD COLUMN embedding VECTOR(1536);

-- ì„ë² ë”© ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
CREATE VECTOR INDEX section_embedding_index 
FOR (s:SECTION) ON (s.embedding)
OPTIONS {indexConfig: {
  `vector.dimensions`: 1536,
  `vector.similarity_function`: 'cosine'
}};

CREATE VECTOR INDEX content_embedding_index 
FOR (c:CONTENT) ON (c.embedding)
OPTIONS {indexConfig: {
  `vector.dimensions`: 1536,
  `vector.similarity_function`: 'cosine'
}};
```

#### 2. ì„ë² ë”© ìƒì„± ì‹œìŠ¤í…œ
```python
class EmbeddingGenerator:
    def __init__(self, openai_client):
        self.openai = openai_client
        self.model = "text-embedding-3-small"
    
    async def generate_embedding(self, text: str) -> list:
        """í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ë²¡í„° ìƒì„±"""
        response = await self.openai.embeddings.create(
            model=self.model,
            input=text,
            encoding_format="float"
        )
        return response.data[0].embedding
    
    async def update_node_embeddings(self, node_type: str, batch_size: int = 100):
        """ë…¸ë“œ ì„ë² ë”© ì¼ê´„ ì—…ë°ì´íŠ¸"""
        offset = 0
        while True:
            # ë…¸ë“œ ë°°ì¹˜ ì¡°íšŒ
            query = f"""
            MATCH (n:{node_type})
            WHERE n.embedding IS NULL
            RETURN n.id as id, n.text as text, n.summary as summary
            ORDER BY n.created_at
            SKIP {offset} LIMIT {batch_size}
            """
            
            results = await self.neo4j.run_query(query)
            if not results:
                break
            
            # ì„ë² ë”© ìƒì„± ë° ì—…ë°ì´íŠ¸
            for result in results:
                text_content = result['text'] or result['summary'] or ""
                if text_content:
                    embedding = await self.generate_embedding(text_content)
                    
                    update_query = f"""
                    MATCH (n:{node_type} {{id: $id}})
                    SET n.embedding = $embedding
                    """
                    
                    await self.neo4j.run_query(update_query, {
                        'id': result['id'],
                        'embedding': embedding
                    })
            
            offset += batch_size
```

#### 3. ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì¿¼ë¦¬
```cypher
-- ì˜ë¯¸ ê²€ìƒ‰ ì¿¼ë¦¬ (ì„¹ì…˜)
CALL db.index.vector.queryNodes('section_embedding_index', 10, $query_embedding)
YIELD node as s, score
MATCH (s)-[:HAS_ELEMENT]->(e:ELEMENT)-[:HAS_CONTENT]->(c:CONTENT)
RETURN s.title, s.summary, score, collect(c.text)[0..3] as sample_content
ORDER BY score DESC
LIMIT 5;

-- ì˜ë¯¸ ê²€ìƒ‰ ì¿¼ë¦¬ (ì½˜í…ì¸ )
CALL db.index.vector.queryNodes('content_embedding_index', 10, $query_embedding)
YIELD node as c, score
MATCH (s:SECTION)-[:HAS_ELEMENT]->(e:ELEMENT)-[:HAS_CONTENT]->(c)
RETURN c.text, c.type, s.title as section_title, score
ORDER BY score DESC
LIMIT 10;
```

#### 4. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
```python
class HybridSearchSystem:
    def __init__(self, neo4j_service, embedding_generator):
        self.neo4j = neo4j_service
        self.embedding_gen = embedding_generator
    
    async def hybrid_search(self, query: str, domain: str, top_k: int = 10) -> dict:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: í‚¤ì›Œë“œ + ì„ë² ë”© ê²€ìƒ‰"""
        # 1. ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = await self.embedding_gen.generate_embedding(query)
        
        # 2. í‚¤ì›Œë“œ ê²€ìƒ‰
        keyword_results = await self.keyword_search(query, domain)
        
        # 3. ì„ë² ë”© ê²€ìƒ‰
        embedding_results = await self.embedding_search(query_embedding, domain)
        
        # 4. ê²°ê³¼ í†µí•© ë° ì¬ì •ë ¬
        combined_results = self.combine_results(keyword_results, embedding_results)
        
        return {
            'query': query,
            'keyword_results': keyword_results,
            'embedding_results': embedding_results,
            'combined_results': combined_results
        }
    
    async def keyword_search(self, query: str, domain: str) -> list:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰"""
        cypher_query = """
        MATCH (root:ROOT {domain: $domain})-[:HAS_DEPTH*1..3]->(depth:DEPTH)
              -[:HAS_SECTION]->(s:SECTION)
              -[:HAS_ELEMENT]->(e:ELEMENT)
              -[:HAS_CONTENT]->(c:CONTENT)
        WHERE c.text CONTAINS $query OR s.title CONTAINS $query
        RETURN s.title, s.summary, c.text, 'keyword' as search_type
        ORDER BY s.importance_score DESC
        LIMIT 10
        """
        
        return await self.neo4j.run_query(cypher_query, {
            'domain': domain,
            'query': query
        })
    
    async def embedding_search(self, query_embedding: list, domain: str) -> list:
        """ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰"""
        cypher_query = """
        MATCH (root:ROOT {domain: $domain})-[:HAS_DEPTH*1..3]->(depth:DEPTH)
              -[:HAS_SECTION]->(s:SECTION)
        WHERE s.embedding IS NOT NULL
        CALL db.index.vector.queryNodes('section_embedding_index', 10, $query_embedding)
        YIELD node as similar_section, score
        WHERE similar_section = s
        RETURN s.title, s.summary, score, 'embedding' as search_type
        ORDER BY score DESC
        LIMIT 10
        """
        
        return await self.neo4j.run_query(cypher_query, {
            'domain': domain,
            'query_embedding': query_embedding
        })
    
    def combine_results(self, keyword_results: list, embedding_results: list) -> list:
        """ê²€ìƒ‰ ê²°ê³¼ í†µí•©"""
        # ê²°ê³¼ ì •ê·œí™” ë° ì ìˆ˜ ê³„ì‚°
        combined = []
        
        # í‚¤ì›Œë“œ ê²°ê³¼ (ì •í™•ë„ ë†’ìŒ)
        for result in keyword_results:
            combined.append({
                'title': result['s.title'],
                'summary': result['s.summary'],
                'score': 1.0,  # í‚¤ì›Œë“œ ë§¤ì¹˜ëŠ” ë†’ì€ ì ìˆ˜
                'type': 'keyword'
            })
        
        # ì„ë² ë”© ê²°ê³¼ (ì˜ë¯¸ì  ìœ ì‚¬ì„±)
        for result in embedding_results:
            combined.append({
                'title': result['s.title'],
                'summary': result['s.summary'],
                'score': result['score'] * 0.8,  # ì„ë² ë”© ì ìˆ˜ ì¡°ì •
                'type': 'embedding'
            })
        
        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        unique_results = self.deduplicate_by_title(combined)
        return sorted(unique_results, key=lambda x: x['score'], reverse=True)
    
    def deduplicate_by_title(self, results: list) -> list:
        """ì œëª© ê¸°ì¤€ ì¤‘ë³µ ì œê±°"""
        seen_titles = set()
        unique_results = []
        
        for result in results:
            if result['title'] not in seen_titles:
                seen_titles.add(result['title'])
                unique_results.append(result)
        
        return unique_results
```

---

## ì¿¼ë¦¬ íŒ¨í„´ ë° ì„±ëŠ¥

### ğŸ” ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ íŒ¨í„´

#### 1. ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ì¡°íšŒ
```cypher
-- ì „ì²´ ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡° ì¡°íšŒ
MATCH path = (root:ROOT)-[:HAS_DEPTH*1..3]->(depth:DEPTH)
             -[:HAS_SECTION]->(section:SECTION)
WHERE root.domain = $domain
RETURN path
ORDER BY depth.level, section.order
LIMIT 100
```

#### 2. í˜ì´ì§•ëœ ì„¹ì…˜ ê²€ìƒ‰
```cypher
-- í˜ì´ì§•ëœ ì„¹ì…˜ ì°¾ê¸°
MATCH (s:SECTION)
WHERE s.is_paginated = true 
  AND s.total_pages > $min_pages
  AND s.summary CONTAINS $search_term
RETURN s.title, s.summary, s.total_pages, s.sampled_pages
ORDER BY s.importance_score DESC
LIMIT 10
```

#### 3. ì½˜í…ì¸  íƒìƒ‰ ê²½ë¡œ
```cypher
-- íŠ¹ì • ì½˜í…ì¸ ê¹Œì§€ì˜ ê²½ë¡œ ì°¾ê¸°
MATCH path = (root:ROOT)-[:HAS_DEPTH*1..3]->(depth:DEPTH)
             -[:HAS_SECTION]->(section:SECTION)
             -[:HAS_ELEMENT]->(element:ELEMENT)
             -[:HAS_CONTENT]->(content:CONTENT)
WHERE root.domain = $domain
  AND content.text CONTAINS $search_term
RETURN path, length(path) as path_length
ORDER BY path_length, content.accessibility_score DESC
LIMIT 5
```

#### 4. ìœ ì‚¬ ì½˜í…ì¸  ì°¾ê¸°
```cypher
-- ìœ ì‚¬í•œ ì„¹ì…˜ ì°¾ê¸°
MATCH (s1:SECTION)-[:SIMILAR_TO]->(s2:SECTION)
WHERE s1.id = $section_id
  AND s2.similarity_score > $threshold
RETURN s2.title, s2.description, s2.similarity_score
ORDER BY s2.similarity_score DESC
```

### âš¡ ì„±ëŠ¥ ìµœì í™” ì¿¼ë¦¬

#### 1. ì¸ë±ìŠ¤ í™œìš© ì¿¼ë¦¬
```cypher
-- ì¸ë±ìŠ¤ë¥¼ í™œìš©í•œ ë¹ ë¥¸ ê²€ìƒ‰
USING INDEX root:ROOT(domain)
MATCH (root:ROOT)
WHERE root.domain = $domain
RETURN root
```

#### 2. ì œí•œëœ ê¹Šì´ íƒìƒ‰
```cypher
-- ê¹Šì´ ì œí•œìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
MATCH path = (root:ROOT)-[:HAS_DEPTH*1..2]->(depth:DEPTH)
WHERE root.domain = $domain
WITH path, depth
ORDER BY depth.importance_score DESC
LIMIT 50
RETURN path
```

#### 3. ì§‘ê³„ ì¿¼ë¦¬ ìµœì í™”
```cypher
-- íš¨ìœ¨ì ì¸ í†µê³„ ì¿¼ë¦¬
MATCH (root:ROOT {domain: $domain})
       -[:HAS_DEPTH]->(depth:DEPTH)
       -[:HAS_SECTION]->(section:SECTION)
RETURN 
  count(DISTINCT depth) as total_pages,
  count(DISTINCT section) as total_sections,
  count(DISTINCT section) FILTER (WHERE section.is_paginated = true) as paginated_sections,
  avg(section.importance_score) as avg_importance
```

---

## í™•ì¥ì„± ê³ ë ¤ì‚¬í•­

### ğŸŒ ìˆ˜í‰ì  í™•ì¥

#### 1. ìƒ¤ë”© ì „ëµ
```python
class Neo4jShardManager:
    def __init__(self, shard_configs: List[Dict]):
        self.shards = {}
        for config in shard_configs:
            self.shards[config['name']] = Neo4jConnection(config)
    
    def get_shard_for_domain(self, domain: str) -> str:
        """ë„ë©”ì¸ ê¸°ë°˜ ìƒ¤ë“œ ì„ íƒ"""
        # ë„ë©”ì¸ í•´ì‹œ ê¸°ë°˜ ìƒ¤ë”©
        hash_value = hash(domain) % len(self.shards)
        return list(self.shards.keys())[hash_value]
    
    async def distributed_query(self, query: str, params: Dict) -> List[Dict]:
        """ë¶„ì‚° ì¿¼ë¦¬ ì‹¤í–‰"""
        tasks = []
        for shard_name, connection in self.shards.items():
            task = asyncio.create_task(
                connection.run_query(query, params)
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return self.merge_results(results)
```

#### 2. ìºì‹± ì „ëµ
```python
class CacheManager:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.cache_ttl = {
            'section_summary': 3600,    # 1ì‹œê°„
            'page_structure': 1800,     # 30ë¶„
            'search_results': 300,      # 5ë¶„
            'navigation_path': 900      # 15ë¶„
        }
    
    async def get_cached_section(self, section_id: str) -> Optional[Dict]:
        """ìºì‹œëœ ì„¹ì…˜ ì •ë³´ ì¡°íšŒ"""
        cache_key = f"section:{section_id}"
        cached_data = await self.redis.get(cache_key)
        
        if cached_data:
            return json.loads(cached_data)
        return None
    
    async def cache_section(self, section_id: str, data: Dict):
        """ì„¹ì…˜ ì •ë³´ ìºì‹±"""
        cache_key = f"section:{section_id}"
        await self.redis.setex(
            cache_key,
            self.cache_ttl['section_summary'],
            json.dumps(data)
        )
```

### ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

#### 1. ì¿¼ë¦¬ ì„±ëŠ¥ ì¶”ì 
```python
class QueryPerformanceMonitor:
    def __init__(self):
        self.query_stats = {}
        self.slow_query_threshold = 1.0  # 1ì´ˆ
    
    async def execute_with_monitoring(self, query: str, params: Dict):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ê³¼ í•¨ê»˜ ì¿¼ë¦¬ ì‹¤í–‰"""
        start_time = time.time()
        
        try:
            result = await neo4j_service.run_query(query, params)
            execution_time = time.time() - start_time
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.update_stats(query, execution_time)
            
            # ëŠë¦° ì¿¼ë¦¬ ë¡œê¹…
            if execution_time > self.slow_query_threshold:
                logger.warning(f"Slow query detected: {execution_time:.2f}s")
                logger.warning(f"Query: {query}")
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Query failed after {execution_time:.2f}s: {str(e)}")
            raise
    
    def update_stats(self, query: str, execution_time: float):
        """ì¿¼ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸"""
        query_hash = hash(query)
        
        if query_hash not in self.query_stats:
            self.query_stats[query_hash] = {
                'count': 0,
                'total_time': 0.0,
                'avg_time': 0.0,
                'max_time': 0.0,
                'min_time': float('inf')
            }
        
        stats = self.query_stats[query_hash]
        stats['count'] += 1
        stats['total_time'] += execution_time
        stats['avg_time'] = stats['total_time'] / stats['count']
        stats['max_time'] = max(stats['max_time'], execution_time)
        stats['min_time'] = min(stats['min_time'], execution_time)
```

### ğŸ”„ ë°ì´í„° ì •í•©ì„±

#### 1. íŠ¸ëœì­ì…˜ ê´€ë¦¬
```python
class TransactionManager:
    def __init__(self, neo4j_service):
        self.neo4j = neo4j_service
    
    async def atomic_section_creation(self, section_data: Dict, 
                                    related_pages: List[Dict]):
        """ì„¹ì…˜ ìƒì„±ì˜ ì›ìì  ì²˜ë¦¬"""
        async with self.neo4j.transaction() as tx:
            try:
                # 1. ì„¹ì…˜ ìƒì„±
                section_query = """
                CREATE (s:SECTION {
                  id: $id,
                  title: $title,
                  type: $type,
                  created_at: datetime()
                })
                RETURN s
                """
                section_result = await tx.run(section_query, section_data)
                
                # 2. ê´€ë ¨ í˜ì´ì§€ë“¤ê³¼ ì—°ê²°
                for page_data in related_pages:
                    relation_query = """
                    MATCH (s:SECTION {id: $section_id})
                    MATCH (d:DEPTH {id: $page_id})
                    CREATE (d)-[:GROUPED_BY]->(s)
                    """
                    await tx.run(relation_query, {
                        'section_id': section_data['id'],
                        'page_id': page_data['id']
                    })
                
                # 3. í†µê³„ ì—…ë°ì´íŠ¸
                stats_query = """
                MATCH (s:SECTION {id: $section_id})
                       <-[:GROUPED_BY]-(d:DEPTH)
                SET s.content_count = count(d)
                """
                await tx.run(stats_query, {'section_id': section_data['id']})
                
                await tx.commit()
                
            except Exception as e:
                await tx.rollback()
                raise e
```

---

## ğŸ¯ ë§ˆë¬´ë¦¬

ì´ ë¬¸ì„œëŠ” Vowser MCP ì„œë²„ì˜ Neo4j ë°ì´í„°ë² ì´ìŠ¤ ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ì¢…í•©ì ì¸ ì„¤ê³„ ë¬¸ì„œì…ë‹ˆë‹¤. 

### ğŸ“‹ ì£¼ìš” íŠ¹ì§• ìš”ì•½

1. **ê³„ì¸µì  ë…¸ë“œ êµ¬ì¡°**: ROOT â†’ SUBROOT â†’ DEPTH â†’ SECTION â†’ ELEMENT â†’ CONTENT
2. **ìë™ ì„¹ì…˜ ìƒì„±**: ìœ ì‚¬í•œ í˜ì´ì§€ë“¤ì„ ìë™ìœ¼ë¡œ ê·¸ë£¹í•‘
3. **í˜ì´ì§• ìµœì í™”**: ëŒ€ìš©ëŸ‰ ì½˜í…ì¸ ì˜ íš¨ìœ¨ì  ì²˜ë¦¬
4. **ì ‘ê·¼ì„± ì¤‘ì‹¬**: ì‚¬ìš©ì ì¹œí™”ì ì¸ íƒìƒ‰ ì •ë³´ ì œê³µ
5. **í™•ì¥ì„±**: ëŒ€ê·œëª¨ ì›¹ì‚¬ì´íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜

### ğŸš€ êµ¬í˜„ ìˆœì„œ ê¶Œì¥ì‚¬í•­

1. **ê¸°ë³¸ ë…¸ë“œ êµ¬ì¡° êµ¬í˜„** (ROOT, DEPTH, SECTION)
2. **ë‹¨ìˆœ ê´€ê³„ ì„¤ì •** (HAS_DEPTH, HAS_SECTION)
3. **Section ìë™ ìƒì„± ë¡œì§** êµ¬í˜„
4. **í˜ì´ì§• ì²˜ë¦¬** ì‹œìŠ¤í…œ êµ¬í˜„
5. **ì„±ëŠ¥ ìµœì í™”** ì ìš©
6. **í™•ì¥ì„± ê³ ë ¤ì‚¬í•­** ë°˜ì˜

ì´ ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì£¼í”¼í„° ë…¸íŠ¸ë¶ì—ì„œ í”„ë¡œí† íƒ€ì…ì„ êµ¬í˜„í•œ í›„, FastAPIë¡œ ì„œë¹„ìŠ¤í™”í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.