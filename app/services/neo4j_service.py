import os
import json
import hashlib

from datetime import datetime
from urllib.parse import urlparse
from dotenv import load_dotenv, find_dotenv
from langchain_neo4j import Neo4jGraph
from app.services.embedding_service import generate_embedding, create_embedding_text

load_dotenv(find_dotenv())

# Neo4j ê·¸ë˜í”„ ê°ì²´ ì´ˆê¸°í™”
try:
    graph = Neo4jGraph(
        url=os.getenv("NEO4J_URI"),
        username=os.getenv("NEO4J_USERNAME"),
        password=os.getenv("NEO4J_PASSWORD"),
    )
    print("Neo4j Service: Database connection successful.")
except Exception as e:
    print(f"Neo4j Service: Database connection failed. Error: {e}")
    graph = None


def extract_domain(url: str) -> str:
    parsed = urlparse(url)
    return parsed.netloc.replace('www.', '')

def is_base_url(url: str) -> bool:
    parsed = urlparse(url)
    return parsed.path in ['', '/'] and not parsed.query and not parsed.fragment

def create_page_id(url: str, element_data: dict) -> str:
    element_key = (
        element_data.get('primarySelector', '') +
        '_' +
        str(element_data.get('textLabels', []))
    )
    return hashlib.md5(f"{url}_{element_key}".encode()).hexdigest()

def normalize_url(url: str) -> str:
    parsed = urlparse(url)
    normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
    if normalized.endswith('/') and parsed.path != '/':
        normalized = normalized[:-1]
    return normalized

def add_metadata_to_path(path_json):
    """
    ê²½ë¡œ JSONì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
    """
    path_string = json.dumps(path_json['completePath'], sort_keys=True)
    
    path_id = hashlib.md5(
        f"{path_json['startCommand']}_{path_string}".encode()
    ).hexdigest()
    
    path_json['metadata'] = {
        'pathId': path_id,
        'createdAt': datetime.now().isoformat(),
        'lastUsed': datetime.now().isoformat(),
        'usageCount': 1,
        'successRate': 1.0,
        'avgExecutionTime': None,
        'createdBy': 'user_001',
        'status': 'active'  # active, deprecated, broken
    }
    
    for step in path_json['completePath']:
        if step['order'] > 0:
            semantic_text = ' '.join([
                ' '.join(step['semanticData'].get('textLabels', [])),
                step['semanticData'].get('contextText', {}).get('immediate', ''),
                step['semanticData'].get('contextText', {}).get('section', ''),
            ])
            neighbor_text = ' '.join(step['semanticData'].get('contextText', {}).get('neighbor', []))
            if neighbor_text:
                semantic_text = f"{semantic_text} {neighbor_text}"

            page_title = step['semanticData'].get('pageInfo', {}).get('title', '')
            if page_title:
                semantic_text = f"{semantic_text} {page_title}"

            semantic_text = ' '.join(semantic_text.split())
            
            step['embeddingText'] = semantic_text
    
    return path_json

def save_path_to_neo4j(path_data):
    """
    ê²½ë¡œ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ Neo4jì— ì €ì¥
    """
    if not graph:
        raise ConnectionError("Neo4j database is not connected.")

    if hasattr(path_data, 'model_dump'):
        path_json = path_data.model_dump()
    else:
        path_json = path_data

    clicks = [step for step in path_json['completePath'] if step['order'] > 0]

    if not clicks:
        print("í´ë¦­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return {"status": "noop", "reason": "No clickable steps found."}

    previous_node_id = None
    previous_node_type = None
    previous_domain = None

    for i, step in enumerate(clicks):
        current_url = step['url']
        current_domain = extract_domain(current_url)
        domain_changed = previous_domain and previous_domain != current_domain

        has_click_element = 'locationData' in step and 'semanticData' in step

        if has_click_element:
            page_id = create_page_id(current_url, step['locationData'])
            embedding_text = step.get('embeddingText', '')
            embedding = generate_embedding(embedding_text)

            create_page_query = """
            MERGE (p:PAGE {pageId: $pageId})
            SET p.url = $url,
                p.domain = $domain,
                p.primarySelector = $primarySelector,
                p.fallbackSelectors = $fallbackSelectors,
                p.anchorPoint = $anchorPoint,
                p.relativePathFromAnchor = $relativePathFromAnchor,
                p.elementSnapshot = $elementSnapshot,
                p.textLabels = $textLabels,
                p.contextText = $contextText,
                p.actionType = $actionType,
                p.embedding = $embedding,
                p.lastUpdated = datetime()
            RETURN p
            """
            page_params = {
                'pageId': page_id,
                'url': current_url,
                'domain': current_domain,
                'primarySelector': step['locationData']['primarySelector'],
                'fallbackSelectors': step['locationData']['fallbackSelectors'],
                'anchorPoint': step['locationData'].get('anchorPoint', ''),
                'relativePathFromAnchor': step['locationData'].get('relativePathFromAnchor', ''),
                'elementSnapshot': json.dumps(step['locationData'].get('elementSnapshot', {})),
                'textLabels': step['semanticData']['textLabels'],
                'contextText': json.dumps(step['semanticData']['contextText']),
                'actionType': step['semanticData']['actionType'],
                'embedding': embedding
            }

            graph.query(create_page_query, page_params)
            print(f"PAGE: {step['semanticData']['textLabels'][0]} ({current_domain})")

            try:
                if i == 0 or domain_changed:
                    create_root_if_needed = """
                    MERGE (r:ROOT {domain: $domain})
                    SET r.baseURL = $baseURL,
                        r.lastVisited = datetime()
                    RETURN r
                    """
                    graph.query(create_root_if_needed, {
                        'domain': current_domain,
                        'baseURL': f"https://{current_domain}"
                    })

                    connect_to_root = """
                    MATCH (r:ROOT {domain: $domain})
                    MATCH (p:PAGE {pageId: $pageId})
                    MERGE (r)-[rel:HAS_PAGE]->(p)
                    SET rel.weight = coalesce(rel.weight, 0) + 1,
                        rel.createdAt = coalesce(rel.createdAt, datetime()),
                        rel.lastUpdated = datetime()
                    """
                    graph.query(connect_to_root, {
                        'domain': current_domain,
                        'pageId': page_id
                    })

                    if previous_node_type == 'PAGE' and domain_changed:
                        cross_connect = """
                        MATCH (p1:PAGE {pageId: $prevId})
                        MATCH (p2:PAGE {pageId: $currId})
                        MERGE (p1)-[rel:NAVIGATES_TO_CROSS_DOMAIN]->(p2)
                        SET rel.weight = coalesce(rel.weight, 0) + 1,
                            rel.createdAt = coalesce(rel.createdAt, datetime()),
                            rel.lastUpdated = datetime()
                        """
                        graph.query(cross_connect, {
                            'prevId': previous_node_id,
                            'currId': page_id
                        })

                elif previous_node_type == 'PAGE':
                    connect_pages = """
                    MATCH (p1:PAGE {pageId: $prevId})
                    MATCH (p2:PAGE {pageId: $currId})
                    MERGE (p1)-[rel:NAVIGATES_TO]->(p2)
                    SET rel.weight = coalesce(rel.weight, 0) + 1,
                        rel.createdAt = coalesce(rel.createdAt, datetime()),
                        rel.lastUpdated = datetime()
                    """
                    graph.query(connect_pages, {
                        'prevId': previous_node_id,
                        'currId': page_id
                    })
            except Exception as e:
                print(f"ê´€ê³„ ìƒì„± ì‹¤íŒ¨: {e}")

            previous_node_type = 'PAGE'
            previous_node_id = page_id

        elif is_base_url(current_url):
            print(f"ROOT: {current_domain} (í´ë¦­ ìš”ì†Œ ì—†ìŒ)")

        previous_domain = current_domain

    # PATH ì—”í‹°í‹° ìƒì„±
    if path_json:
        path_id = create_path_entity(path_json)
        if path_id:
            print(f"PATH ì—”í‹°í‹° ìƒì„±ë¨: {path_id}")

    print("\nê²½ë¡œ ì €ì¥ ì™„ë£Œ!")
    return {"status": "success", "saved_steps": len(clicks)}


def save_page_analysis_to_neo4j(url: str, page_structure):
    """
    í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ Neo4jì— ì €ì¥
    
    Args:
        url (str): ë¶„ì„í•œ í˜ì´ì§€ URL
        page_structure: PageStructure ê°ì²´ (Pydantic ëª¨ë¸)
    """
    if not graph:
        raise ConnectionError("Neo4j database is not connected.")
    
    domain = extract_domain(url)
    
    try:
        analysis_id = hashlib.md5(f"{url}_{page_structure.page_title}".encode()).hexdigest()
        
        create_analysis_query = """
        MERGE (pa:PAGE_ANALYSIS {analysisId: $analysisId})
        SET pa.url = $url,
            pa.domain = $domain,
            pa.pageTitle = $pageTitle,
            pa.analyzedAt = datetime(),
            pa.sectionsCount = $sectionsCount
        RETURN pa
        """
        
        analysis_params = {
            'analysisId': analysis_id,
            'url': url,
            'domain': domain,
            'pageTitle': page_structure.page_title,
            'sectionsCount': len(page_structure.sections)
        }
        
        graph.query(create_analysis_query, analysis_params)
        print(f"PAGE_ANALYSIS: {page_structure.page_title} ({domain})")
        
        graph.query(
            "MERGE (r:ROOT {domain: $domain}) SET r.lastAnalyzed = datetime()",
            {'domain': domain}
        )
        
        graph.query(
            """
            MATCH (r:ROOT {domain: $domain})
            MATCH (pa:PAGE_ANALYSIS {analysisId: $analysisId})
            MERGE (r)-[rel:HAS_ANALYSIS]->(pa)
            SET rel.weight = coalesce(rel.weight, 0) + 1,
                rel.createdAt = coalesce(rel.createdAt, datetime()),
                rel.lastUpdated = datetime()
            """,
            {'domain': domain, 'analysisId': analysis_id}
        )
        
        for i, section in enumerate(page_structure.sections):
            section_id = f"{analysis_id}_section_{i}"
            
            create_section_query = """
            MERGE (s:SECTION {sectionId: $sectionId})
            SET s.sectionName = $sectionName,
                s.elementCount = $elementCount,
                s.order = $order
            RETURN s
            """
            
            section_params = {
                'sectionId': section_id,
                'sectionName': section.section_name,
                'elementCount': len(section.elements),
                'order': i
            }
            
            graph.query(create_section_query, section_params)
            
            graph.query(
                """
                MATCH (pa:PAGE_ANALYSIS {analysisId: $analysisId})
                MATCH (s:SECTION {sectionId: $sectionId})
                MERGE (pa)-[rel:HAS_SECTION]->(s)
                SET rel.order = $order
                """,
                {'analysisId': analysis_id, 'sectionId': section_id, 'order': i}
            )
            
            for j, element in enumerate(section.elements):
                element_id = f"{section_id}_element_{j}"
                
                embedding = generate_embedding(element.text)
                
                create_element_query = """
                MERGE (e:ELEMENT {elementId: $elementId})
                SET e.text = $text,
                    e.actionType = $actionType,
                    e.embedding = $embedding,
                    e.order = $order
                RETURN e
                """
                
                element_params = {
                    'elementId': element_id,
                    'text': element.text,
                    'actionType': element.action_type,
                    'embedding': embedding,
                    'order': j
                }
                
                graph.query(create_element_query, element_params)
                
                graph.query(
                    """
                    MATCH (s:SECTION {sectionId: $sectionId})
                    MATCH (e:ELEMENT {elementId: $elementId})
                    MERGE (s)-[rel:HAS_ELEMENT]->(e)
                    SET rel.order = $order
                    """,
                    {'sectionId': section_id, 'elementId': element_id, 'order': j}
                )
        
        print(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {len(page_structure.sections)}ê°œ ì„¹ì…˜, "
              f"{sum(len(s.elements) for s in page_structure.sections)}ê°œ ìš”ì†Œ")
        
        return {
            "status": "success", 
            "analysisId": analysis_id,
            "sectionsCount": len(page_structure.sections),
            "elementsCount": sum(len(s.elements) for s in page_structure.sections)
        }
        
    except Exception as e:
        print(f"í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return {"status": "error", "message": str(e)}


def check_graph_structure():
    if not graph:
        print("Neo4j database is not connected.")
        return
        
    stats_query = """
    MATCH (r:ROOT)
    MATCH (p:PAGE)
    OPTIONAL MATCH (r)-[hp:HAS_PAGE]->()
    OPTIONAL MATCH ()-[nt:NAVIGATES_TO]->()
    OPTIONAL MATCH ()-[ntcd:NAVIGATES_TO_CROSS_DOMAIN]->()
    RETURN
        count(DISTINCT r) as root_count,
        count(DISTINCT p) as page_count,
        count(DISTINCT hp) as has_page_count,
        count(DISTINCT nt) as navigates_to_count,
        count(DISTINCT ntcd) as cross_domain_count
    """

    try:
        result = graph.query(stats_query)
        print("ê·¸ë˜í”„ í†µê³„:")
        for r in result:
            print(f"  - ROOT ë…¸ë“œ: {r['root_count']}")
            print(f"  - PAGE ë…¸ë“œ: {r['page_count']}")
            print(f"  - HAS_PAGE ê´€ê³„: {r['has_page_count']}")
            print(f"  - NAVIGATES_TO ê´€ê³„: {r['navigates_to_count']}")
            print(f"  - CROSS_DOMAIN ê´€ê³„: {r['cross_domain_count']}")
        return result[0] if result else None
    except Exception as e:
        print(f"ê·¸ë˜í”„ êµ¬ì¡° í™•ì¸ ì‹¤íŒ¨: {e}")
        return None

def visualize_paths(domain: str):
    """íŠ¹ì • ë„ë©”ì¸ì˜ ê²½ë¡œ ì‹œê°í™”"""
    if not graph:
        print("Neo4j database is not connected.")
        return
        
    query = """
    MATCH path = (r:ROOT {domain: $domain})-[:HAS_PAGE|NAVIGATES_TO*]->(p:PAGE)
    WITH r, p, length(path) as depth
    RETURN
        depth,
        p.url as url,
        p.textLabels as labels,
        p.primarySelector as selector
    ORDER BY depth
    LIMIT 20
    """

    try:
        results = graph.query(query, {'domain': domain})

        print(f"\n{domain}ì—ì„œ ì‹œì‘í•˜ëŠ” ê²½ë¡œ:")
        if not results:
            print("ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        current_depth = 0
        for r in results:
            if r['depth'] != current_depth:
                current_depth = r['depth']
                print(f"\n{'  ' * (current_depth-1)}ğŸ“ ê¹Šì´ {current_depth}:")
            labels = r['labels'] if r['labels'] else ['N/A']
            print(f"{'  ' * current_depth}â†’ {labels[0]}")
            print(f"{'  ' * current_depth}  URL: {r['url']}")
            
        return results
    except Exception as e:
        print(f"ê²½ë¡œ ì‹œê°í™” ì‹¤íŒ¨: {e}")
        return None



def find_popular_paths(domain: str = None, limit: int = 10):
    """ì¸ê¸° ê²½ë¡œ ì°¾ê¸°"""
    if not graph:
        print("Neo4j database is not connected.")
        return
        
    if domain:
        query = """
        MATCH (r:ROOT {domain: $domain})-[rel:HAS_PAGE]->(p:PAGE)
        RETURN 
            p.textLabels as labels,
            p.url as url,
            rel.weight as popularity,
            rel.lastUsed as lastUsed
        ORDER BY rel.weight DESC
        LIMIT $limit
        """
        params = {'domain': domain, 'limit': limit}
    else:
        query = """
        MATCH (r:ROOT)-[rel:HAS_PAGE]->(p:PAGE)
        RETURN 
            r.domain as domain,
            p.textLabels as labels,
            p.url as url,
            rel.weight as popularity,
            rel.lastUsed as lastUsed
        ORDER BY rel.weight DESC
        LIMIT $limit
        """
        params = {'limit': limit}

    try:
        results = graph.query(query, params)
        
        domain_text = f" ({domain})" if domain else ""
        print(f"\nì¸ê¸° ê²½ë¡œ{domain_text}:")
        if not results:
            print("ì¸ê¸° ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        for i, r in enumerate(results, 1):
            labels = r['labels'] if r['labels'] else ['N/A']
            domain_info = f" - {r['domain']}" if not domain else ""
            print(f"  {i}. {labels[0]}{domain_info}")
            print(f"     ì¸ê¸°ë„: {r['popularity']}, ë§ˆì§€ë§‰ ì‚¬ìš©: {r['lastUsed']}")
            print(f"     URL: {r['url']}")
            print()
            
        return results
    except Exception as e:
        print(f"ì¸ê¸° ê²½ë¡œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None


def run_overlap_test(test_cases: list = None):
    if not graph:
        print("Neo4j database is not connected.")
        return
        
    print("ê²½ë¡œ ê²¹ì¹¨ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")

    if test_cases:
        for i, test_case in enumerate(test_cases, 1):
            print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}: {test_case['startCommand']}")
            test_with_metadata = add_metadata_to_path(test_case.copy())
            save_path_to_neo4j(test_with_metadata)
            print()

    print("\ní…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„:")

    try:
        # 1. ê³µìœ ë˜ëŠ” ë…¸ë“œ í™•ì¸
        shared_nodes = graph.query("""
        MATCH (p:PAGE)<-[:HAS_PAGE|NAVIGATES_TO*]-(r:ROOT)
        WITH p, count(DISTINCT r) as root_count
        WHERE root_count > 0
        MATCH (p)<-[rel:HAS_PAGE|NAVIGATES_TO]-()
        WITH p, count(rel) as incoming_count
        WHERE incoming_count > 1
        RETURN p.textLabels[0] as page, p.url as url, incoming_count
        """)

        print("\nê³µìœ ë˜ëŠ” PAGE ë…¸ë“œ:")
        if shared_nodes:
            for node in shared_nodes:
                print(f"  - {node['page']}: {node['incoming_count']}ê°œ ê²½ë¡œì—ì„œ ì‚¬ìš©")
        else:
            print("  ê³µìœ ë˜ëŠ” ë…¸ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

        # 2. ê²½ë¡œ ê°€ì¤‘ì¹˜ í™•ì¸
        weights = graph.query("""
        MATCH ()-[rel:HAS_PAGE|NAVIGATES_TO]->()
        WHERE rel.weight > 1
        MATCH (from)-[rel]->(to)
        RETURN
            labels(from)[0] as from_type,
            CASE
                WHEN from:ROOT THEN from.domain
                ELSE from.textLabels[0]
            END as from_name,
            labels(to)[0] as to_type,
            CASE
                WHEN to:ROOT THEN to.domain
                ELSE to.textLabels[0]
            END as to_name,
            rel.weight as weight
        ORDER BY rel.weight DESC
        """)

        print("\nì¦ê°€ëœ ê°€ì¤‘ì¹˜ (2íšŒ ì´ìƒ ì‚¬ìš©ëœ ê²½ë¡œ):")
        if weights:
            for w in weights:
                print(f"  - {w['from_name']} â†’ {w['to_name']}: ê°€ì¤‘ì¹˜ {w['weight']}")
        else:
            print("  ê°€ì¤‘ì¹˜ê°€ ì¦ê°€ëœ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.")

        # 3. ì „ì²´ ê²½ë¡œ ì‹œê°í™” (YouTube ê¸°ì¤€)
        print("\nì „ì²´ ê²½ë¡œ êµ¬ì¡°:")
        all_paths = graph.query("""
        MATCH (r:ROOT {domain: 'youtube.com'})
        MATCH path = (r)-[:HAS_PAGE|NAVIGATES_TO*..5]->(end:PAGE)
        WHERE NOT (end)-[:NAVIGATES_TO]->()
        RETURN [n in nodes(path) |
            CASE
                WHEN n:ROOT THEN n.domain
                ELSE n.textLabels[0]
            END
        ] as path_nodes
        LIMIT 10
        """)

        if all_paths:
            for i, path in enumerate(all_paths, 1):
                print(f"  ê²½ë¡œ {i}: {' â†’ '.join(path['path_nodes'])}")
        else:
            print("  YouTube ë„ë©”ì¸ì˜ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.")

        return {
            "shared_nodes": shared_nodes,
            "weighted_paths": weights,
            "path_visualizations": all_paths
        }

    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None


# PATH ì—”í‹°í‹° ê´€ë ¨ í•¨ìˆ˜ë“¤ ì¶”ê°€

def create_path_entity(path_json):
    """
    ê²½ë¡œ ì €ì¥ í›„ PATH ì—”í‹°í‹° ìƒì„±
    """
    if not graph:
        raise ConnectionError("Neo4j database is not connected.")
    
    try:
        # ê²½ë¡œì˜ ëª¨ë“  ë…¸ë“œ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ì„¤ëª… ìƒì„±
        clicks = [step for step in path_json['completePath'] if step['order'] > 0]
        if not clicks:
            return None
            
        # ë…¸ë“œ ì‹œí€€ìŠ¤ ìƒì„±
        node_sequence = ["root_" + extract_domain(clicks[0]['url'])]
        descriptions = []
        
        for step in clicks:
            if 'locationData' in step and 'semanticData' in step:
                page_id = create_page_id(step['url'], step['locationData'])
                node_sequence.append("page_" + page_id)
                
                # í…ìŠ¤íŠ¸ ë¼ë²¨ë¡œ ì„¤ëª… ìƒì„±
                labels = step['semanticData'].get('textLabels', [])
                if labels:
                    descriptions.append(labels[0])
        
        # PATH ì„¤ëª… ìƒì„±
        path_description = " â†’ ".join(descriptions)
        
        # ì „ì²´ ê²½ë¡œì˜ ì„ë² ë”© ìƒì„±
        full_text = path_json.get('startCommand', '') + " " + path_description
        path_embedding = generate_embedding(full_text)
        
        # PATH ID ìƒì„±
        path_id = "path_" + hashlib.md5("_".join(node_sequence).encode()).hexdigest()
        
        # PATH ì—”í‹°í‹° ìƒì„±
        create_path_query = """
        MERGE (path:PATH {pathId: $pathId})
        SET path.description = $description,
            path.nodeSequence = $nodeSequence,
            path.startDomain = $startDomain,
            path.targetPageId = $targetPageId,
            path.embedding = $embedding,
            path.totalWeight = coalesce(path.totalWeight, 0) + 1,
            path.usageCount = coalesce(path.usageCount, 0) + 1,
            path.createdAt = coalesce(path.createdAt, datetime()),
            path.lastUsed = datetime(),
            path.lastUpdated = datetime()
        RETURN path
        """
        
        path_params = {
            'pathId': path_id,
            'description': path_description,
            'nodeSequence': node_sequence,
            'startDomain': extract_domain(clicks[0]['url']),
            'targetPageId': node_sequence[-1] if node_sequence else None,
            'embedding': path_embedding
        }
        
        result = graph.query(create_path_query, path_params)
        
        # PATHì™€ PAGE ë…¸ë“œ ì—°ê²°
        if len(node_sequence) > 1:  # ROOT ì œì™¸í•˜ê³  PAGEë“¤ë§Œ
            page_ids = [nid.replace("page_", "") for nid in node_sequence[1:]]
            connect_query = """
            MATCH (path:PATH {pathId: $pathId})
            MATCH (p:PAGE) WHERE p.pageId IN $pageIds
            MERGE (path)-[:CONTAINS]->(p)
            """
            graph.query(connect_query, {'pathId': path_id, 'pageIds': page_ids})
        
        print(f"PATH ì—”í‹°í‹° ìƒì„±: {path_description}")
        return path_id
        
    except Exception as e:
        print(f"PATH ì—”í‹°í‹° ìƒì„± ì‹¤íŒ¨: {e}")
        return None


def search_paths_by_query(query_text, limit=3, domain_hint=None):
    """
    ìì—°ì–´ ì¿¼ë¦¬ë¡œ ê´€ë ¨ ê²½ë¡œ ê²€ìƒ‰
    """
    print(f"[DEBUG] search_paths_by_query ì‹œì‘: {query_text}")
    
    if not graph:
        print("[DEBUG] Neo4j ì—°ê²° ì—†ìŒ")
        raise ConnectionError("Neo4j database is not connected.")
    
    import time
    start_time = time.time()
    
    try:
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        print(f"[DEBUG] ì„ë² ë”© ìƒì„± ì¤‘...")
        query_embedding = generate_embedding(query_text)
        if not query_embedding:
            print("[DEBUG] ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            return None
        print(f"[DEBUG] ì„ë² ë”© ìƒì„± ì„±ê³µ: ì°¨ì› {len(query_embedding)}")
        
        # ë„ë©”ì¸ íŒíŠ¸ ì²˜ë¦¬
        domain_filter = ""
        if domain_hint:
            domain_filter = f"AND path.startDomain = '{domain_hint}'"
        elif "ìœ íŠœë¸Œ" in query_text or "youtube" in query_text.lower():
            domain_filter = "AND path.startDomain = 'youtube.com'"
        
        # 1. PATH ì—”í‹°í‹°ì—ì„œ ë²¡í„° ê²€ìƒ‰ (Pythonì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°)
        path_search_query = f"""
        MATCH (path:PATH)
        WHERE path.embedding IS NOT NULL {domain_filter}
        RETURN path
        """
        
        print(f"[DEBUG] PATH ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘...")
        all_paths = graph.query(path_search_query)
        print(f"[DEBUG] ì°¾ì€ PATH ìˆ˜: {len(all_paths)}")
        
        # Pythonì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        import numpy as np
        
        def cosine_similarity(vec1, vec2):
            vec1 = np.array(vec1)
            vec2 = np.array(vec2)
            return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
        
        path_results = []
        for path_data in all_paths:
            path = path_data['path']
            if path.get('embedding'):
                similarity = cosine_similarity(query_embedding, path['embedding'])
                print(f"[DEBUG] PATH {path.get('pathId', 'unknown')}: ìœ ì‚¬ë„ {similarity:.3f}")
                if similarity > 0.5:  # ì„ê³„ê°’ì„ 0.7ì—ì„œ 0.5ë¡œ ë‚®ì¶¤
                    path_results.append({
                        'path': path,
                        'similarity': similarity
                    })
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        path_results = sorted(path_results, key=lambda x: x['similarity'], reverse=True)[:limit]
        print(f"[DEBUG] 0.7 ì´ìƒ PATH ìˆ˜: {len(path_results)}")
        
        # 2. PATHê°€ ì—†ìœ¼ë©´ PAGE ë…¸ë“œì—ì„œ ê²€ìƒ‰
        if not path_results:
            print("PATH ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ, PAGE ë…¸ë“œì—ì„œ ê²€ìƒ‰")
            page_search_query = f"""
            MATCH (page:PAGE)
            WHERE page.embedding IS NOT NULL
            WITH page, gds.similarity.cosine(page.embedding, $queryEmbedding) as similarity
            WHERE similarity > 0.7
            ORDER BY similarity DESC
            LIMIT 5
            """
            
            page_results = graph.query(page_search_query, {
                'queryEmbedding': query_embedding
            })
            
            if page_results:
                # ì°¾ì€ PAGEë¥¼ í¬í•¨í•˜ëŠ” ê²½ë¡œ êµ¬ì„±
                path_results = []
                for page_result in page_results:
                    page = page_result['page']
                    # í•´ë‹¹ PAGEê¹Œì§€ì˜ ê²½ë¡œ ì°¾ê¸°
                    path_query = """
                    MATCH path = (root:ROOT)-[:HAS_PAGE|NAVIGATES_TO*]->(target:PAGE {pageId: $pageId})
                    RETURN path, $similarity as similarity
                    LIMIT 1
                    """
                    paths = graph.query(path_query, {
                        'pageId': page['pageId'],
                        'similarity': page_result['similarity']
                    })
                    if paths:
                        path_results.extend(paths)
        
        # 3. ê²°ê³¼ í¬ë§·íŒ…
        matched_paths = []
        for result in path_results[:limit]:
            if 'path' in result and hasattr(result['path'], 'nodes'):
                # ê²½ë¡œì—ì„œ ë…¸ë“œ ì¶”ì¶œ
                nodes = result['path'].nodes
                steps = []
                
                for i, node in enumerate(nodes):
                    if 'domain' in node and 'baseURL' in node:  # ROOT ë…¸ë“œ
                        steps.append({
                            'order': i,
                            'type': 'ROOT',
                            'domain': node['domain'],
                            'url': node.get('baseURL', f"https://{node['domain']}"),
                            'action': f"{node['domain']} ì ‘ì†"
                        })
                    elif 'pageId' in node:  # PAGE ë…¸ë“œ
                        action = f"{node.get('textLabels', ['ì‘ì—…'])[0]} í´ë¦­"
                        steps.append({
                            'order': i,
                            'type': 'PAGE',
                            'pageId': node['pageId'],
                            'url': node.get('url', ''),
                            'selector': node.get('primarySelector', ''),
                            'anchorPoint': node.get('anchorPoint', ''),
                            'action': action,
                            'textLabels': node.get('textLabels', [])
                        })
                
                if steps:
                    # ì‹œê°„ ê°ì‡  ê³„ì‚°
                    time_decay = 1.0
                    if 'lastUsed' in result.get('path', {}):
                        days_old = (datetime.now() - result['path']['lastUsed']).days
                        time_decay = max(0.5, 1.0 - (days_old / 30) * 0.5)
                    
                    # ìµœì¢… ì ìˆ˜ ê³„ì‚°
                    total_weight = result.get('path', {}).get('totalWeight', 1)
                    relevance_score = (
                        result['similarity'] * 0.5 +
                        min(total_weight / 100, 1.0) * 0.3 +
                        time_decay * 0.2
                    )
                    
                    matched_paths.append({
                        'pathId': result.get('path', {}).get('pathId', 'unknown'),
                        'relevance_score': round(relevance_score, 3),
                        'total_weight': total_weight,
                        'last_used': result.get('path', {}).get('lastUsed'),
                        'estimated_time': result.get('path', {}).get('avgExecutionTime'),
                        'steps': steps
                    })
            elif 'path' in result:
                # PATH ì—”í‹°í‹°ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¨ ê²½ìš°
                path_entity = result['path']
                # ë…¸ë“œ ì‹œí€€ìŠ¤ë¡œë¶€í„° ê²½ë¡œ ì¬êµ¬ì„±
                steps = reconstruct_path_from_sequence(path_entity['nodeSequence'])
                
                if steps:
                    matched_paths.append({
                        'pathId': path_entity['pathId'],
                        'relevance_score': round(result['similarity'], 3),
                        'total_weight': path_entity.get('totalWeight', 1),
                        'last_used': path_entity.get('lastUsed'),
                        'estimated_time': path_entity.get('avgExecutionTime'),
                        'steps': steps
                    })
        
        search_time_ms = int((time.time() - start_time) * 1000)
        
        return {
            'query': query_text,
            'matched_paths': matched_paths,
            'search_metadata': {
                'total_found': len(matched_paths),
                'search_time_ms': search_time_ms,
                'vector_search_used': True,
                'min_score_threshold': 0.7
            }
        }
        
    except Exception as e:
        print(f"ê²½ë¡œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return None


def reconstruct_path_from_sequence(node_sequence):
    """
    ë…¸ë“œ ì‹œí€€ìŠ¤ë¡œë¶€í„° ê²½ë¡œ ì¬êµ¬ì„±
    """
    if not graph or not node_sequence:
        return []
    
    steps = []
    for i, node_id in enumerate(node_sequence):
        if node_id.startswith("root_"):
            domain = node_id.replace("root_", "")
            steps.append({
                'order': i,
                'type': 'ROOT',
                'title': f"{domain} ë©”ì¸",
                'domain': domain,
                'url': f"https://{domain}",
                'action': f"{domain} ì ‘ì†"
            })
        elif node_id.startswith("page_"):
            page_id = node_id.replace("page_", "")
            # PAGE ë…¸ë“œ ì •ë³´ ì¡°íšŒ
            page_query = """
            MATCH (p:PAGE {pageId: $pageId})
            RETURN p
            """
            page_results = graph.query(page_query, {'pageId': page_id})
            
            if page_results:
                page = page_results[0]['p']
                action = f"{page.get('textLabels', ['ì‘ì—…'])[0]} í´ë¦­"
                title = page.get('textLabels', [''])[0] if page.get('textLabels') else action.replace(' í´ë¦­', '')
                steps.append({
                    'order': i,
                    'type': 'PAGE',
                    'title': title,
                    'pageId': page_id,
                    'url': page.get('url', ''),
                    'selector': page.get('primarySelector', ''),
                    'anchorPoint': page.get('anchorPoint', ''),
                    'action': action,
                    'textLabels': page.get('textLabels', [])
                })
    
    return steps


def update_path_usage(path_id):
    """
    ê²½ë¡œ ì‚¬ìš© ì‹œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
    """
    if not graph:
        return
    
    try:
        # PATH ì—”í‹°í‹° ì—…ë°ì´íŠ¸
        update_path_query = """
        MATCH (path:PATH {pathId: $pathId})
        SET path.totalWeight = path.totalWeight + 1,
            path.usageCount = path.usageCount + 1,
            path.lastUsed = datetime()
        """
        graph.query(update_path_query, {'pathId': path_id})
        
        # ê²½ë¡œìƒì˜ ëª¨ë“  ê´€ê³„ ì—…ë°ì´íŠ¸
        update_relations_query = """
        MATCH (path:PATH {pathId: $pathId})-[:CONTAINS]->(page:PAGE)
        MATCH paths = (:ROOT)-[rels:HAS_PAGE|NAVIGATES_TO*]->(page)
        FOREACH (rel IN rels |
            SET rel.weight = coalesce(rel.weight, 0) + 1,
                rel.lastUpdated = datetime()
        )
        """
        graph.query(update_relations_query, {'pathId': path_id})
        
        print(f"ê²½ë¡œ ì‚¬ìš© ì¶”ì  ì™„ë£Œ: {path_id}")
        
    except Exception as e:
        print(f"ê²½ë¡œ ì‚¬ìš© ì¶”ì  ì‹¤íŒ¨: {e}")


def cleanup_old_paths():
    """
    30ì¼ ì´ìƒ ë¯¸ì‚¬ìš© ê²½ë¡œ ì •ë¦¬
    """
    if not graph:
        return
    
    try:
        # 1. 30ì¼ ì´ìƒ ë¯¸ì‚¬ìš© PATH weight ê°ì†Œ
        decay_query = """
        MATCH (path:PATH)
        WHERE path.lastUsed < datetime() - duration('P30D')
        SET path.totalWeight = CASE 
            WHEN path.totalWeight > 0 THEN path.totalWeight - 1
            ELSE 0
        END
        RETURN count(path) as decayed_count
        """
        decay_result = graph.query(decay_query)
        
        # 2. weightê°€ 0ì¸ PATH ì‚­ì œ
        delete_paths_query = """
        MATCH (path:PATH)
        WHERE path.totalWeight <= 0
        WITH path, path.pathId as pathId
        DETACH DELETE path
        RETURN count(pathId) as deleted_paths
        """
        delete_result = graph.query(delete_paths_query)
        
        # 3. weightê°€ 0ì¸ ê´€ê³„ ì‚­ì œ
        delete_relations_query = """
        MATCH ()-[rel:HAS_PAGE|NAVIGATES_TO|NAVIGATES_TO_CROSS_DOMAIN]->()
        WHERE rel.weight <= 0
        DELETE rel
        RETURN count(rel) as deleted_relations
        """
        rel_result = graph.query(delete_relations_query)
        
        # 4. ê³ ë¦½ëœ PAGE ë…¸ë“œ ì‚­ì œ
        cleanup_pages_query = """
        MATCH (page:PAGE)
        WHERE NOT (page)<-[:HAS_PAGE|NAVIGATES_TO]-()
          AND NOT (page)-[:NAVIGATES_TO]->()
          AND NOT (:PATH)-[:CONTAINS]->(page)
        DELETE page
        RETURN count(page) as deleted_pages
        """
        page_result = graph.query(cleanup_pages_query)
        
        print(f"ì •ë¦¬ ì™„ë£Œ:")
        print(f"  - ê°€ì¤‘ì¹˜ ê°ì†Œ: {decay_result[0]['decayed_count']}ê°œ ê²½ë¡œ")
        print(f"  - ì‚­ì œëœ PATH: {delete_result[0]['deleted_paths']}ê°œ")
        print(f"  - ì‚­ì œëœ ê´€ê³„: {rel_result[0]['deleted_relations']}ê°œ")
        print(f"  - ì‚­ì œëœ PAGE: {page_result[0]['deleted_pages']}ê°œ")
        
        return {
            'decayed_paths': decay_result[0]['decayed_count'],
            'deleted_paths': delete_result[0]['deleted_paths'],
            'deleted_relations': rel_result[0]['deleted_relations'],
            'deleted_pages': page_result[0]['deleted_pages']
        }
        
    except Exception as e:
        print(f"ê²½ë¡œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return None


def create_vector_indexes():
    """
    ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
    """
    if not graph:
        return
    
    try:
        # PAGE ë…¸ë“œ ë²¡í„° ì¸ë±ìŠ¤
        page_index_query = """
        CREATE VECTOR INDEX page_embedding IF NOT EXISTS
        FOR (p:PAGE) ON (p.embedding)
        OPTIONS {indexConfig: {
            `vector.dimensions`: 1536,
            `vector.similarity_function`: 'cosine'
        }}
        """
        graph.query(page_index_query)
        print("PAGE ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        # PATH ë…¸ë“œ ë²¡í„° ì¸ë±ìŠ¤
        path_index_query = """
        CREATE VECTOR INDEX path_embedding IF NOT EXISTS
        FOR (p:PATH) ON (p.embedding)
        OPTIONS {indexConfig: {
            `vector.dimensions`: 1536,
            `vector.similarity_function`: 'cosine'
        }}
        """
        graph.query(path_index_query)
        print("PATH ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        # ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤
        text_indexes = [
            """CREATE FULLTEXT INDEX page_text_search IF NOT EXISTS
               FOR (p:PAGE) ON EACH [p.textLabels]""",
            """CREATE FULLTEXT INDEX path_description_search IF NOT EXISTS
               FOR (p:PATH) ON EACH [p.description]"""
        ]
        
        for index_query in text_indexes:
            graph.query(index_query)
        print("ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        print(f"ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        return False