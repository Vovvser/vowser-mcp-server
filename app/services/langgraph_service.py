"""
LangGraph ì„œë¹„ìŠ¤ - ì¡°ê±´ë¶€ ë¶„ê¸°ë¥¼ í†µí•œ ì§€ëŠ¥ì  ê²½ë¡œ ì„ íƒ

ìƒˆë¡œìš´ í”Œë¡œìš°:
ì‚¬ìš©ì ìš”ì²­ â†’ ì˜ë„ ë¶„ì„ â†’ ë²¡í„° ìœ ì‚¬ë„ ë¶„ì„ â†’ {
    ìœ ì‚¬ë„ < 0.43: ë‹¤ë¥¸ Agentë¡œ ê²½ë¡œ ì¬íƒìƒ‰
    ìœ ì‚¬ë„ >= 0.43: ê¸°ì¡´ ê²½ë¡œ top k ìˆœìœ„í™”
} â†’ ìµœì¢… ê²½ë¡œ ì„ íƒ
"""

import json
import time
from typing import TypedDict, List, Optional
import os
import re
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI

from app.services import neo4j_service
from app.services.embedding_service import generate_embedding


class PathSelectionState(TypedDict):
    """State for conditional path selection workflow"""
    user_query: str
    domain_hint: Optional[str]
    query_embedding: List[float]
    intent_analysis: dict  # ì˜ë„ ë¶„ì„ ê²°ê³¼
    similarity_threshold: float  # ë²¡í„° ìœ ì‚¬ë„ ì„ê³„ê°’
    max_similarity: float  # ìµœëŒ€ ìœ ì‚¬ë„ ì ìˆ˜
    selected_paths: List[dict]  # ìµœì¢… ì„ íƒëœ ê²½ë¡œë“¤
    processing_strategy: str  # ì‚¬ìš©ëœ ì²˜ë¦¬ ì „ëµ
    reasoning: str
    limit: int  # ë°˜í™˜í•  ê²½ë¡œ ìˆ˜
    cached_search_results: Optional[dict]  # ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ (ì¤‘ë³µ ê²€ìƒ‰ ë°©ì§€)
    
# Util í•¨ìˆ˜
def parse_llm_json(text: str) -> dict:
    # 1) ë°©ì–´ì  ì •ë¦¬: ì–‘ë ê³µë°±, BOM ì œê±°
    s = text.lstrip("\ufeff").strip()

    # 2) ì½”ë“œíœìŠ¤ ì œê±° (```json ... ```, ``` ... ```)
    if s.startswith("```"):
        # ì²« ì¤„ì´ ```json ë˜ëŠ” ``` ì¸ ê²½ìš°ë§Œ ì˜ë¼ë‚´ê¸°
        # (ë§¨ ëì˜ ``` ì œê±°)
        s = re.sub(r"^```[a-zA-Z0-9]*\s*", "", s)
        s = re.sub(r"\s*```$", "", s)

    # 3) ë¬¸ìì—´ ì „ì²´ê°€ JSONì´ ì•„ë‹ ìˆ˜ë„ ìˆìœ¼ë‹ˆ {...} ë¸”ë¡ë§Œ ì¶”ì¶œ
    #    - ê°€ì¥ ì•ì˜ '{'ì™€ ê°€ì¥ ë’¤ì˜ '}' ì‚¬ì´ë¥¼ ì˜ë¼ì„œ ì‹œë„
    if not (s.startswith("{") and s.endswith("}")):
        start = s.find("{")
        end = s.rfind("}")
        if start != -1 and end != -1 and end > start:
            s = s[start:end+1]

    return json.loads(s)

# ============================================================================
# LangGraph ì›Œí¬í”Œë¡œìš° ë…¸ë“œ êµ¬í˜„
# ============================================================================

def _debug_node_execution(node_name: str, state: dict, is_start: bool = True):
    """ë…¸ë“œ ì‹¤í–‰ ë””ë²„ê¹… ì¶œë ¥"""
    action = "ì‹œì‘" if is_start else "ì™„ë£Œ"
    print(f"ğŸ” [{node_name}] {action}")
    if is_start:
        print(f"   ì…ë ¥ ìƒíƒœ: {list(state.keys())}")
    else:
        print(f"   ì¶œë ¥ ìƒíƒœ: {list(state.keys())}")
    print("-" * 50)


def _debug_edge_transition(from_node: str, to_node: str, condition: str = None):
    """ì—£ì§€ ì „í™˜ ë””ë²„ê¹… ì¶œë ¥"""
    if condition:
        print(f"â¡ï¸  [{from_node}] â†’ [{to_node}] (ì¡°ê±´: {condition})")
    else:
        print(f"â¡ï¸  [{from_node}] â†’ [{to_node}]")
    print("-" * 50)


async def analyze_user_intent(state: PathSelectionState) -> PathSelectionState:
    """
    ì‚¬ìš©ì ì¿¼ë¦¬ì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
    
    ë¶„ì„ í•­ëª©:
    - ì˜ë„ ìœ í˜•: navigation, task_completion, information_seeking, exploration
    - ë„ë©”ì¸ ì„ í˜¸ë„: íŠ¹ì • ì‚¬ì´íŠ¸ë‚˜ ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì–¸ê¸‰
    - ë³µì¡ë„: ë‹¨ìˆœí•œ ì‘ì—…ì¸ì§€ ë³µí•©ì ì¸ ì‘ì—…ì¸ì§€
    - ê¸´ê¸‰ë„: ì¦‰ì‹œ ì‹¤í–‰ì´ í•„ìš”í•œ ì‘ì—…ì¸ì§€
    """
    
    # í™˜ê²½ ë³€ìˆ˜ ì—†ìœ¼ë©´ LLM ìƒëµ
    use_llm = bool(os.getenv("OPENAI_API_KEY"))
    print(f"ğŸ”§ LLM ì‚¬ìš© ì—¬ë¶€: {use_llm}")
    
    if not use_llm:
        print("âš ï¸  OPENAI_API_KEYê°€ ì—†ì–´ì„œ íœ´ë¦¬ìŠ¤í‹± í´ë°± ì‚¬ìš©")
        result = {
            "intent_type": "information_seeking",
            "domain_preference": None,
            "complexity": "simple",
            "confidence": 0.6,
            "reasoning": "Heuristic fallback without LLM",
            "keywords": [state["user_query"]]  # ê¸°ë³¸ì ìœ¼ë¡œ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
        }
    else:
        llm = ChatOpenAI(
            model="gpt-4o-mini", 
            temperature=0, 
            max_retries=2,
            request_timeout=10.0  # 10ì´ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        )
        
        prompt = f"""
        ë‹¹ì‹ ì€ ì›¹ ìë™í™” ì„œë¹„ìŠ¤ì˜ ì˜ë„ ë¶„ì„ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
        ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ë„ ìœ í˜•, ë„ë©”ì¸ ì„ í˜¸ë„, ì‘ì—… ë³µì¡ë„, ì‹ ë¢°ë„ ì ìˆ˜, í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

        ë¶„ì„í•  í•­ëª©:
        1. ì˜ë„ ìœ í˜• (navigation, task_completion, information_seeking, exploration)
        2. ë„ë©”ì¸ ì„ í˜¸ë„ (íŠ¹ì • ì‚¬ì´íŠ¸ ì–¸ê¸‰ ì—¬ë¶€)
        3. ì‘ì—… ë³µì¡ë„ (simple, moderate, complex)
        4. ì‹ ë¢°ë„ ì ìˆ˜ (0.0-1.0)
        5. í•µì‹¬ í‚¤ì›Œë“œ (ê²€ìƒ‰ì— ì‚¬ìš©í•  í•µì‹¬ ë‹¨ì–´ë“¤)

        ì˜ˆì‹œ:
        
        query: "ìœ íŠœë¸Œì—ì„œ ì¢‹ì•„ìš” ëˆ„ë¥´ê¸°"
        response: {{
            "intent_type": "task_completion",
            "domain_preference": "youtube.com",
            "complexity": "simple",
            "confidence": 0.85,
            "reasoning": "ì‚¬ìš©ìê°€ ìœ íŠœë¸Œì—ì„œ íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ë ¤ëŠ” ì˜ë„ê°€ ëª…í™•í•¨",
            "keywords": ["ìœ íŠœë¸Œ", "ì¢‹ì•„ìš”", "ëˆ„ë¥´ê¸°", "ë™ì˜ìƒ"]
        }}

        query: "ë‚ ì”¨ê°€ ë„ˆë¬´ ì¶”ì›Œìš”"
        response: {{
            "intent_type": "information_seeking",
            "domain_preference": null,
            "complexity": "simple",
            "confidence": 0.75,
            "reasoning": "ì‚¬ìš©ìê°€ ë‚ ì”¨ ì •ë³´ë¥¼ ì°¾ê³  ìˆìŒ",
            "keywords": ["ë‚ ì”¨", "ì¶”ìœ„", "ì˜¨ë„", "ê¸°ì˜¨"]
        }}

        query: "ìš”ì¦˜ ë‚˜ë¼ê°€ ì–´ë–»ê²Œ êµ´ëŸ¬ê°€ë‚˜"
        response: {{
            "intent_type": "navigation",
            "domain_preference": "naver.com",
            "complexity": "simple",
            "confidence": 0.90,
            "reasoning": "ìµœì‹  êµ­ë‚´ ì •ì¹˜, ì‚¬íšŒ ì´ìŠˆë¥¼ ë³´ë ¤ëŠ” ì˜ë„ë¡œ ë³´ì„.",
            "keywords": ["ì‹œì‚¬", "ì •ì¹˜", "ë‰´ìŠ¤", "ìµœê·¼ ì´ìŠˆ"]
        }}

        ì´ì œ ë‹¤ìŒ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
        query: "{state['user_query']}"
        response:"""

        try:
            print("ğŸ¤– LLM í˜¸ì¶œ ì¤‘...")
            import asyncio
            
            # asyncio.wait_forë¡œ íƒ€ì„ì•„ì›ƒ ì„¤ì • (12ì´ˆ - LLM ìì²´ íƒ€ì„ì•„ì›ƒ 10ì´ˆ + ì—¬ìœ  2ì´ˆ)
            response = await asyncio.wait_for(
                llm.ainvoke(prompt),
                timeout=12.0
            )
            print(f"ğŸ“ LLM ì‘ë‹µ: {response.content}")
            result = parse_llm_json(response.content)
        except asyncio.TimeoutError:
            print(f"âŒ LLM í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (12ì´ˆ)")
            result = {
                "intent_type": "information_seeking",
                "domain_preference": None,
                "complexity": "simple",
                "confidence": 0.5,
                "reasoning": "LLM íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¸í•œ í´ë°±",
                "keywords": [state["user_query"]]
            }
        except Exception as e:
            print(f"âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            result = {
                "intent_type": "information_seeking",
                "domain_preference": None,
                "complexity": "simple",
                "confidence": 0.5,
                "reasoning": f"LLM ì‹¤íŒ¨ë¡œ ì¸í•œ í´ë°±: {str(e)}",
                "keywords": [state["user_query"]]  # ê¸°ë³¸ì ìœ¼ë¡œ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
            }

    output_state = {
        **state,
        "intent_analysis": result,
        "query_embedding": generate_embedding(state["user_query"])
    }
    
    return output_state


async def analyze_vector_similarity(state: PathSelectionState) -> PathSelectionState:
    """
    ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë²¡í„° ìœ ì‚¬ë„ ë¶„ì„í•˜ì—¬ ë¶„ê¸° ê²°ì •
    
    ë¶„ì„ ê³¼ì •:
    1. ê¸°ì¡´ ê²€ìƒ‰ìœ¼ë¡œ ìµœëŒ€ ìœ ì‚¬ë„ ì ìˆ˜ í™•ì¸ (limitë§Œí¼ ê²€ìƒ‰í•˜ì—¬ ìºì‹±)
    2. ì„ê³„ê°’ê³¼ ë¹„êµí•˜ì—¬ ë¶„ê¸° ì „ëµ ê²°ì •
    3. ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
    
    ìµœì í™”: rank_existing_pathsì—ì„œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê²€ìƒ‰ ê²°ê³¼ ìºì‹±
    """
    
    # ê¸°ì¡´ ê²€ìƒ‰ìœ¼ë¡œ ê²°ê³¼ í™•ì¸ (ìš”ì²­ëœ limitë§Œí¼ ê²€ìƒ‰í•˜ì—¬ ìºì‹±)
    existing_results = neo4j_service.search_paths_by_query(
        state["user_query"],
        limit=state.get("limit", 3),  # ìš”ì²­ëœ ê°œìˆ˜ë§Œí¼ ê²€ìƒ‰
        domain_hint=state["domain_hint"]
    )
    
    max_similarity = 0.0
    if existing_results and existing_results["matched_paths"]:
        max_similarity = existing_results["matched_paths"][0].get("relevance_score", 0.0)
        print(f"ğŸ“Š ë°œê²¬ëœ ê²½ë¡œ ìˆ˜: {len(existing_results['matched_paths'])}")
        print(f"ğŸ“Š ê²½ë¡œ ì´ë¦„: {existing_results['matched_paths'][0].get('taskIntent')}")
        print(f"ğŸ“Š ìµœëŒ€ ìœ ì‚¬ë„: {max_similarity:.3f}")
    else:
        print("ğŸ“Š ê¸°ì¡´ ê²½ë¡œ ì—†ìŒ")
    
    # ì„ê³„ê°’ ì„¤ì • (0.43)
    similarity_threshold = 0.43
    
    output_state = {
        **state,
        "max_similarity": max_similarity,
        "similarity_threshold": similarity_threshold,
        "cached_search_results": existing_results  # ê²€ìƒ‰ ê²°ê³¼ ìºì‹±
    }
    
    return output_state


def should_use_rediscovery_agent(state: PathSelectionState) -> str:
    """
    ë²¡í„° ìœ ì‚¬ë„ì— ë”°ë¼ ë¶„ê¸° ê²°ì •
    
    Returns:
    - "high_similarity": ê¸°ì¡´ ê²½ë¡œ ìˆœìœ„í™” ì‚¬ìš©
    - "low_similarity": ë‹¤ë¥¸ Agentë¡œ ì¬íƒìƒ‰ ì‚¬ìš©
    """
    max_similarity = state["max_similarity"]
    threshold = state["similarity_threshold"]
    
    print(f"ğŸ”€ ë¶„ê¸° ê²°ì •: ìœ ì‚¬ë„ {max_similarity:.3f} vs ì„ê³„ê°’ {threshold}")
    
    if max_similarity >= threshold:
        decision = "high_similarity"
        print(f"âœ… ë†’ì€ ìœ ì‚¬ë„ â†’ rank_existing_paths")
    else:
        decision = "low_similarity"
        print(f"âš ï¸  ë‚®ì€ ìœ ì‚¬ë„ â†’ analyze_intent")
    
    return decision


async def rank_existing_paths(state: PathSelectionState) -> PathSelectionState:
    """
    ë†’ì€ ìœ ì‚¬ë„ê°€ í™•ì¸ëœ ê²½ìš° ê¸°ì¡´ ê²½ë¡œë“¤ì„ ìˆœìœ„í™”
    
    ë†’ì€ ìœ ì‚¬ë„ì¼ ë•ŒëŠ” ì˜ë„ ë¶„ì„ ì—†ì´ ê¸°ì¡´ ê²½ë¡œë§Œ ë°˜í™˜
    
    ìµœì í™”: analyze_vector_similarityì—ì„œ ìºì‹±ëœ ê²€ìƒ‰ ê²°ê³¼ ì¬ì‚¬ìš©
    """
    
    # ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš© (ì¤‘ë³µ Neo4j ì¿¼ë¦¬ ë°©ì§€)
    existing_results = state.get("cached_search_results")
    
    if not existing_results:
        print("âŒ ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŒ")
        output_state = {
            **state,
            "selected_paths": [],
            "processing_strategy": "rank_existing_paths",
            "reasoning": "ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ì„œ ë¹ˆ ê²°ê³¼ ë°˜í™˜"
        }
        
        return output_state
    
    print(f"ğŸ“Š ìºì‹œëœ ê²½ë¡œ ì‚¬ìš©: {len(existing_results['matched_paths'])}ê°œ")
    
    # ë†’ì€ ìœ ì‚¬ë„ì¼ ë•ŒëŠ” ê¸°ì¡´ ê²½ë¡œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì˜ë„ ë¶„ì„ ì—†ì´)
    selected_paths = existing_results["matched_paths"]
    
    # ê° ê²½ë¡œì— ê¸°ë³¸ ì ìˆ˜ ì •ë³´ ì¶”ê°€
    for i, path in enumerate(selected_paths):
        print(f"  {i+1}. {path.get('taskIntent', 'Unknown')} - ì ìˆ˜: {path.get('relevance_score', 0):.3f}")
    
    output_state = {
        **state,
        "selected_paths": selected_paths,
        "processing_strategy": "rank_existing_paths",
        "reasoning": f"ë†’ì€ ìœ ì‚¬ë„({state['max_similarity']:.3f})ë¡œ ìºì‹œëœ ê²½ë¡œ ì‚¬ìš© (ì¤‘ë³µ ê²€ìƒ‰ ì œê±°)"
    }
    
    print(f"âœ… ìµœì¢… ì„ íƒëœ ê²½ë¡œ ìˆ˜: {len(output_state['selected_paths'])}")
    return output_state


async def rediscover_with_different_agent(state: PathSelectionState) -> PathSelectionState:
    """
    ë‚®ì€ ìœ ì‚¬ë„ ìƒí™©ì—ì„œ ë‹¤ë¥¸ Agent ì „ëµìœ¼ë¡œ ê²½ë¡œ ì¬íƒìƒ‰ (ìµœì í™”)
    
    ë‹¤ë¥¸ Agent ì „ëµ:
    1. í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ Agent (ë‹¨ì¼ Agentë¡œ ìµœì í™”)
    """
    
    intent_analysis = state["intent_analysis"]
    print(f"ğŸ” ë‚®ì€ ìœ ì‚¬ë„ë¡œ ë‹¤ë¥¸ Agent ì „ëµ ì‚¬ìš© (ìµœëŒ€ ìœ ì‚¬ë„: {state['max_similarity']:.3f})")
    print(f"ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {intent_analysis.get('keywords', [])}")
    
    rediscovered_paths = []
    
    # Agent 1: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ Agent (ë‹¨ì¼ Agentë¡œ ìµœì í™”)
    print("ğŸ¤– í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ Agent ì‹¤í–‰")
    keyword_agent_paths = await keyword_based_search_agent(state)
    rediscovered_paths.extend(keyword_agent_paths)
    print(f"ğŸ“Š ì´ ì¬íƒìƒ‰ëœ ê²½ë¡œ: {len(rediscovered_paths)}ê°œ")
    
    # ì¤‘ë³µ ì œê±° (ê°„ë‹¨í•œ ë°©ì‹)
    unique_paths = []
    seen_intents = set()
    for path in rediscovered_paths:
        intent_key = f"{path.get('domain', '')}_{path.get('taskIntent', '')}"
        if intent_key not in seen_intents:
            seen_intents.add(intent_key)
            unique_paths.append(path)
    
    print(f"ğŸ“Š ì¤‘ë³µ ì œê±° í›„: {len(unique_paths)}ê°œ")
    
    # ì ìˆ˜ ì¬ê³„ì‚° (ê°„ë‹¨í•œ ë°©ì‹)
    scored_paths = []
    for i, path in enumerate(unique_paths):
        # ê¸°ë³¸ ì ìˆ˜ì— ì•½ê°„ì˜ ë³´ë„ˆìŠ¤ë§Œ ì¶”ê°€
        base_score = path.get("relevance_score", 0.0)
        path["rediscovery_score"] = base_score + 0.1  # ê°„ë‹¨í•œ ë³´ë„ˆìŠ¤
        scored_paths.append(path)
    
    # ì ìˆ˜ë¡œ ì •ë ¬
    scored_paths.sort(key=lambda x: x["rediscovery_score"], reverse=True)
    
    # í´ë¼ì´ì–¸íŠ¸ê°€ ëª¨ë¥´ëŠ” í•„ë“œ ì œê±° í›„ ë°˜í™˜
    forbidden = {"agent_source", "rediscovery_score", "composite_score"}
    cleaned_paths = [
        {k: v for k, v in p.items() if k not in forbidden}
        for p in scored_paths
    ]

    output_state = {
        **state,
        "selected_paths": cleaned_paths[:state.get("limit", 3)],
        "processing_strategy": "rediscover_with_different_agent",
        "reasoning": f"ë‚®ì€ ìœ ì‚¬ë„({state['max_similarity']:.3f})ë¡œ í‚¤ì›Œë“œ ê¸°ë°˜ Agent ì‚¬ìš©"
    }
    
    print(f"âœ… ìµœì¢… ì„ íƒëœ ê²½ë¡œ ìˆ˜: {len(output_state['selected_paths'])}")
    return output_state


# ============================================================================
# ë‹¤ì¤‘ Agent êµ¬í˜„
# ============================================================================

async def keyword_based_search_agent(state: PathSelectionState) -> List[dict]:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ Agent (ìµœì í™” - ë³‘ë ¬ ê²€ìƒ‰)"""
    import asyncio
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ ë° í™•ì¥
    keywords = extract_and_expand_keywords(state["user_query"], state["intent_analysis"])
    
    # ë³‘ë ¬ ê²€ìƒ‰ì„ ìœ„í•œ ë¹„ë™ê¸° í•¨ìˆ˜
    async def search_keyword(keyword: str) -> List[dict]:
        try:
            # Neo4j ê²€ìƒ‰ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ (blocking -> non-blocking)
            loop = asyncio.get_event_loop()
            results = await loop.run_in_executor(
                None,
                lambda: neo4j_service.search_paths_by_query(
                    keyword,
                    limit=1,  # ê° í‚¤ì›Œë“œë‹¹ 1ê°œë§Œ ê°€ì ¸ì˜¤ê¸°
                    domain_hint=None  # ë„ë©”ì¸ ì œí•œ ì—†ì´ ê²€ìƒ‰
                )
            )
            
            if results and results["matched_paths"]:
                paths = []
                for path in results["matched_paths"]:
                    path["agent_source"] = "keyword_based"
                    paths.append(path)
                return paths
            return []
        except Exception as e:
            print(f"âš ï¸ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
            return []
    
    # ìµœëŒ€ 2ê°œ í‚¤ì›Œë“œë¥¼ ë³‘ë ¬ë¡œ ê²€ìƒ‰
    search_tasks = [search_keyword(keyword) for keyword in keywords[:2]]
    results_lists = await asyncio.gather(*search_tasks)
    
    # ê²°ê³¼ ë³‘í•©
    paths = []
    for result_list in results_lists:
        paths.extend(result_list)
    
    print(f"ğŸ”‘ í‚¤ì›Œë“œ ê¸°ë°˜ ë³‘ë ¬ ê²€ìƒ‰ ì™„ë£Œ: {len(paths)}ê°œ ê²½ë¡œ")
    return paths


async def cross_domain_search_agent(state: PathSelectionState) -> List[dict]:
    """ë„ë©”ì¸ í¬ë¡œìŠ¤ ê²€ìƒ‰ Agent (ìµœì í™”)"""
    intent_analysis = state["intent_analysis"]
    paths = []
    
    # ìœ ì‚¬í•œ ì˜ë„ë¥¼ ê°€ì§„ ë‹¤ë¥¸ ë„ë©”ì¸ ê²€ìƒ‰
    similar_intent_query = generate_cross_domain_query(intent_analysis)
    
    try:
        results = neo4j_service.search_paths_by_query(
            similar_intent_query,
            limit=2,  # 3ê°œì—ì„œ 2ê°œë¡œ ì¤„ì„
            domain_hint=None  # ëª¨ë“  ë„ë©”ì¸ì—ì„œ ê²€ìƒ‰
        )
        
        if results and results["matched_paths"]:
            for path in results["matched_paths"]:
                path["agent_source"] = "cross_domain"
                paths.append(path)
    except Exception as e:
        print(f"âš ï¸ í¬ë¡œìŠ¤ ë„ë©”ì¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    print(f"ğŸŒ í¬ë¡œìŠ¤ ë„ë©”ì¸ ê²€ìƒ‰ ì™„ë£Œ: {len(paths)}ê°œ ê²½ë¡œ")
    return paths

# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ============================================================================

def extract_and_expand_keywords(query: str, intent_analysis: dict) -> List[str]:
    """LLMì´ ì¶”ì¶œí•œ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ê³  í•„ìš”ì‹œ í™•ì¥"""
    # LLMì´ ì¶”ì¶œí•œ í‚¤ì›Œë“œ ìš°ì„  ì‚¬ìš©
    llm_keywords = intent_analysis.get("keywords", [])
    
    if llm_keywords and len(llm_keywords) > 0:
        print(f"ğŸ”‘ LLM ì¶”ì¶œ í‚¤ì›Œë“œ: {llm_keywords}")
        return llm_keywords[:4]  # ìµœëŒ€ 4ê°œë¡œ ì œí•œ
    
    # LLM í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
    print(f"ğŸ”‘ LLM í‚¤ì›Œë“œ ì—†ìŒ, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©: {query}")
    return [query]


def generate_cross_domain_query(intent_analysis: dict) -> str:
    """í¬ë¡œìŠ¤ ë„ë©”ì¸ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„± (ê°„ë‹¨í™”)"""
    # LLMì´ ì¶”ì¶œí•œ í‚¤ì›Œë“œ ìš°ì„  ì‚¬ìš©
    keywords = intent_analysis.get("keywords", [])
    if keywords and len(keywords) > 0:
        # í‚¤ì›Œë“œë“¤ì„ ì¡°í•©í•˜ì—¬ í¬ë¡œìŠ¤ ë„ë©”ì¸ ì¿¼ë¦¬ ìƒì„±
        return " ".join(keywords[:2])  # ìµœëŒ€ 2ê°œ í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ ë‹¨ìˆœí™”
    
    # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì˜ë„ ìœ í˜• ê¸°ë°˜ ìƒì„±
    intent_type = intent_analysis.get("intent_type", "information_seeking")
    return f"{intent_type} ê´€ë ¨ ì‘ì—…"


def deduplicate_paths(paths: List[dict]) -> List[dict]:
    """ì¤‘ë³µ ê²½ë¡œ ì œê±°"""
    seen = set()
    unique_paths = []
    
    for path in paths:
        path_key = f"{path.get('domain', '')}_{path.get('taskIntent', '')}"
        if path_key not in seen:
            seen.add(path_key)
            unique_paths.append(path)
    
    return unique_paths


# ============================================================================
# ì „ì—­ ì›Œí¬í”Œë¡œìš° ìºì‹œ
# ============================================================================

# ì „ì—­ ì›Œí¬í”Œë¡œìš° ìºì‹œ
_langgraph_workflow = None
_workflow_initialized = False

def get_or_build_workflow():
    """ì›Œí¬í”Œë¡œìš°ë¥¼ í•œ ë²ˆë§Œ ë¹Œë“œí•˜ê³  ìºì‹œ"""
    global _langgraph_workflow, _workflow_initialized
    
    if not _workflow_initialized:
        print("LangGraph ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì¤‘...")
        start_time = time.time()
        
        _langgraph_workflow = build_path_selection_graph()
        
        init_time = int((time.time() - start_time) * 1000)
        print(f"LangGraph ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì™„ë£Œ ({init_time}ms)")
        _workflow_initialized = True
    
    return _langgraph_workflow


def initialize_langgraph():
    """ì„œë²„ ì‹œì‘ ì‹œ LangGraph ì›Œí¬í”Œë¡œìš° ë¯¸ë¦¬ ì´ˆê¸°í™”"""
    print("LangGraph ì›Œí¬í”Œë¡œìš° ì‚¬ì „ ì´ˆê¸°í™”...")
    get_or_build_workflow()
    print("LangGraph ì›Œí¬í”Œë¡œìš° ì‚¬ì „ ì´ˆê¸°í™” ì™„ë£Œ")


# ============================================================================
# LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ë° êµ¬ì¡° ì¶œë ¥
# ============================================================================

def build_path_selection_graph():
    """Build conditional LangGraph for path selection"""

    workflow = StateGraph(PathSelectionState)

    # Node: ë²¡í„° ìœ ì‚¬ë„ ë¶„ì„ (ì§„ì…ì )
    workflow.add_node("analyze_similarity", analyze_vector_similarity)

    # Node: ì˜ë„ ë¶„ì„ (ë‚®ì€ ìœ ì‚¬ë„ì¼ ë•Œë§Œ ì‹¤í–‰)
    workflow.add_node("analyze_intent", analyze_user_intent)

    # Node 3: ê¸°ì¡´ ê²½ë¡œ ìˆœìœ„í™” (ë†’ì€ ìœ ì‚¬ë„)
    workflow.add_node("rank_existing_paths", rank_existing_paths)

    # Node 4: ë‹¤ë¥¸ Agentë¡œ ê²½ë¡œ ì¬íƒìƒ‰ (ë‚®ì€ ìœ ì‚¬ë„)
    workflow.add_node("rediscover_with_agent", rediscover_with_different_agent)

    # ì¡°ê±´ë¶€ ë¶„ê¸°: ë²¡í„° ìœ ì‚¬ë„ì— ë”°ë¼ ë‹¤ë¥¸ ì „ëµ ì„ íƒ
    workflow.add_conditional_edges(
        "analyze_similarity",
        should_use_rediscovery_agent,
        {
            "high_similarity": "rank_existing_paths",
            "low_similarity": "analyze_intent"
        }
    )

    # ë‚®ì€ ìœ ì‚¬ë„ íë¦„: ì˜ë„ ë¶„ì„ í›„ ì¬íƒìƒ‰
    workflow.add_edge("analyze_intent", "rediscover_with_agent")

    # ë‘ ê²½ë¡œ ëª¨ë‘ ìµœì¢… ê²°ê³¼ë¡œ ì—°ê²°
    workflow.add_edge("rank_existing_paths", END)
    workflow.add_edge("rediscover_with_agent", END)

    # ì§„ì…ì ì€ ìœ ì‚¬ë„ ë¶„ì„
    workflow.set_entry_point("analyze_similarity")

    return workflow.compile()


def print_langgraph_structure():
    """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¡°ë¥¼ ì¶œë ¥"""
    workflow = get_or_build_workflow()
    
    print("\n" + "="*60)
    print("LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¡°")
    print("="*60)
    
    # ì›Œí¬í”Œë¡œìš° ì •ë³´ ì¶œë ¥
    print(f"Entry Point: analyze_similarity")
    print(f"End Points: END")
    print(f"Total Nodes: 4")
    print(f"Conditional Branches: 1")
    
    print("\në…¸ë“œ êµ¬ì¡°:")
    print("1. analyze_similarity - ë²¡í„° ìœ ì‚¬ë„ ë¶„ì„")
    print("2. analyze_intent - ì‚¬ìš©ì ì˜ë„ ë¶„ì„ (ìœ ì‚¬ë„ < 0.43ì¼ ë•Œë§Œ)")
    print("3. rank_existing_paths - ê¸°ì¡´ ê²½ë¡œ ìˆœìœ„í™” (ìœ ì‚¬ë„ >= 0.43)")
    print("4. rediscover_with_agent - ë‹¤ë¥¸ Agentë¡œ ì¬íƒìƒ‰ (ìœ ì‚¬ë„ < 0.43)")
    
    print("\në¶„ê¸° ì¡°ê±´:")
    print("- ìœ ì‚¬ë„ >= 0.43: rank_existing_paths")
    print("- ìœ ì‚¬ë„ < 0.43: rediscover_with_agent")
    
    print("\nì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„:")
    print("analyze_similarity â†’ {")
    print("    high_similarity: rank_existing_paths â†’ END")
    print("    low_similarity: analyze_intent â†’ rediscover_with_agent â†’ END")
    print("}")
    
    print("\n" + "="*60)
    
    print(workflow.get_graph().draw_ascii())
    
    return workflow


def get_workflow_info():
    """ì›Œí¬í”Œë¡œìš° ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜"""
    return {
        "entry_point": "analyze_similarity",
        "end_points": ["END"],
        "total_nodes": 4,
        "conditional_branches": 1,
        "nodes": {
            "analyze_similarity": "ë²¡í„° ìœ ì‚¬ë„ ë¶„ì„",
            "analyze_intent": "ì‚¬ìš©ì ì˜ë„ ë¶„ì„ (ìœ ì‚¬ë„ < 0.43)", 
            "rank_existing_paths": "ê¸°ì¡´ ê²½ë¡œ ìˆœìœ„í™” (ìœ ì‚¬ë„ >= 0.43)",
            "rediscover_with_agent": "ë‹¤ë¥¸ Agentë¡œ ì¬íƒìƒ‰ (ìœ ì‚¬ë„ < 0.43)"
        },
        "threshold": 0.43,
        "branches": {
            "high_similarity": "rank_existing_paths",
            "low_similarity": "rediscover_with_agent"
        }
    }


# ============================================================================
# ë©”ì¸ ì„œë¹„ìŠ¤ í•¨ìˆ˜
# ============================================================================

async def search_with_langgraph(
    query: str, 
    limit: int = 5,
    domain_hint: Optional[str] = None
) -> dict:
    """
    LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•œ ì§€ëŠ¥ì  ê²½ë¡œ ê²€ìƒ‰
    
    Args:
        query: ì‚¬ìš©ì ìì—°ì–´ ì¿¼ë¦¬
        limit: ìµœëŒ€ ë°˜í™˜ ê²½ë¡œ ìˆ˜
        domain_hint: íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ (ì„ íƒì‚¬í•­)
    
    Returns:
        dict: ê¸°ì¡´ ì‘ë‹µ í˜•ì‹ê³¼ í˜¸í™˜ë˜ëŠ” ê²€ìƒ‰ ê²°ê³¼
    """
    print("ğŸš€ LangGraph ì›Œí¬í”Œë¡œìš° ì‹œì‘")
    print(f"ğŸ“ ì¿¼ë¦¬: {query}, ì œí•œ: {limit}, ë„ë©”ì¸ íŒíŠ¸: {domain_hint}")
    
    start_time = time.time()
    
    try:
        # ìºì‹œëœ ì›Œí¬í”Œë¡œìš° ì‚¬ìš© (ë¹Œë“œ ì‹œê°„ ì ˆì•½)
        workflow = get_or_build_workflow()
        
        initial_state = {
            "user_query": query,
            "domain_hint": domain_hint,
            "limit": limit,
            "query_embedding": [],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
            "intent_analysis": {},  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”
            "similarity_threshold": 0.0,
            "max_similarity": 0.0,
            "selected_paths": [],
            "processing_strategy": "",
            "reasoning": "",
            "cached_search_results": None  # ìºì‹œ ì´ˆê¸°í™”
        }
        
        print("ğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...")
        result = await workflow.ainvoke(initial_state)
        
        processing_time = int((time.time() - start_time) * 1000)
        
        print(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time}ms")
        print(f"ğŸ“Š ë°œê²¬ëœ ê²½ë¡œ ìˆ˜: {len(result.get('selected_paths', []))}")
        
        # ê¸°ì¡´ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        response = {
            "query": result["user_query"],
            "total_matched": len(result["selected_paths"]),
            "matched_paths": result["selected_paths"],
            "performance": {
                "search_time": processing_time,
                "reasoning": result["reasoning"],
                "strategy": result["processing_strategy"],
                "max_similarity": result["max_similarity"]
            }
        }
        
        print("âœ… LangGraph ì›Œí¬í”Œë¡œìš° ì™„ë£Œ")
        return response
        
    except Exception as e:
        print(f"âŒ LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        print("ğŸ”„ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±...")
        
        # ê¸°ì¡´ ê²€ìƒ‰ ë°©ì‹ìœ¼ë¡œ í´ë°±
        fallback_result = neo4j_service.search_paths_by_query(query, limit, domain_hint)
        if fallback_result:
            fallback_result["performance"]["reasoning"] = f"LangGraph ì‹¤íŒ¨ë¡œ í´ë°±: {str(e)}"
            fallback_result["performance"]["strategy"] = "fallback_traditional_search"
        
        print("="*60)
        return fallback_result


def format_langgraph_response(langgraph_result: dict) -> dict:
    """LangGraph ê²°ê³¼ë¥¼ ê¸°ì¡´ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    return {
        "query": langgraph_result["user_query"],
        "total_matched": len(langgraph_result["selected_paths"]),
        "matched_paths": langgraph_result["selected_paths"],
        "performance": {
            "search_time": langgraph_result.get("processing_time", 0),
            "reasoning": langgraph_result.get("reasoning", ""),
            "strategy": langgraph_result.get("processing_strategy", "")
        }
    }


# ============================================================================
# í…ŒìŠ¤íŠ¸ ë° ì‹¤í–‰ì„ ìœ„í•œ Main í•¨ìˆ˜
# ============================================================================

async def test_langgraph_workflow():
    """LangGraph ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
    test_queries = [
        "ì™œ ì´ë ‡ê²Œ ì¶”ì›Œ",
        "SRT ì˜ˆë§¤"
    ]
    
    for i, query in enumerate(test_queries, 1):        
        try:
            result = await search_with_langgraph(query, limit=3)
            
            # print(f"âœ… LangGraphë¡œ ê²€ìƒ‰ ì„±ê³µ!")
            # print(f"ğŸ“Š ì´ {result['total_matched']}ê°œ ê²½ë¡œ ë°œê²¬")
            # print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result['performance']['search_time']}ms")
            # print(f"ğŸ§  ì „ëµ: {result['performance']['strategy']}")
            # print(f"ğŸ’­ ì¶”ë¡ : {result['performance']['reasoning']}")
            # print(f"ğŸ“ˆ ìµœëŒ€ ìœ ì‚¬ë„: {result['performance']['max_similarity']:.3f}")
            
            if result['matched_paths']:
                # print("\nğŸ” ë°œê²¬ëœ ê²½ë¡œë“¤:")
                for j, path in enumerate(result['matched_paths'], 1):
                    print(f"  {j}. {path.get('taskIntent', 'Unknown')}")
                    print(f"     ë„ë©”ì¸: {path.get('domain', 'Unknown')}")
                    print(f"     ê¸°ì¡´ ìœ ì‚¬ë„: {path.get('relevance_score', 0):.3f}")
                    if 'composite_score' in path:
                        print(f"     ë³µí•©ì ìˆ˜: {path['composite_score']:.3f}")
                    if 'rediscovery_score' in path:
                        print(f"     ì¬íƒìƒ‰ ì ìˆ˜: {path['rediscovery_score']:.3f}")
                    if 'agent_source' in path:
                        print(f"     Agent: {path['agent_source']}")
            else:
                print("âŒ ê²½ë¡œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()


async def interactive_test():
    """ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸"""
    print("ğŸ¯ LangGraph ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸")
    
    while True:
        try:
            query = input("\nğŸ” ê²€ìƒ‰í•  ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit'): ").strip()
            
            if query.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ í…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
                
            if not query:
                print("âŒ ë¹ˆ ì¿¼ë¦¬ëŠ” ì…ë ¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            print(f"\nğŸ” ê²€ìƒ‰ ì¤‘: {query}")
            
            result = await search_with_langgraph(query, limit=3)
            
            print(f"âœ… ëŒ€í™”í˜• ê²½ë¡œ ê²€ìƒ‰ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ {result['total_matched']}ê°œ ê²½ë¡œ ë°œê²¬")
            print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {result['performance']['search_time']}ms")
            print(f"ğŸ§  ì „ëµ: {result['performance']['strategy']}")
            print(f"ğŸ’­ ì¶”ë¡ : {result['performance']['reasoning']}")
            print(f"ğŸ“ˆ ìµœëŒ€ ìœ ì‚¬ë„: {result['performance']['max_similarity']:.3f}")
            
            if result['matched_paths']:
                print("\nğŸ” ë°œê²¬ëœ ê²½ë¡œë“¤:")
                for j, path in enumerate(result['matched_paths'], 1):
                    print(f"  {j}. {path.get('taskIntent', 'Unknown')}")
                    print(f"     ë„ë©”ì¸: {path.get('domain', 'Unknown')}")
                    print(f"     ì ìˆ˜: {path.get('relevance_score', 0):.3f}")
                    if 'composite_score' in path:
                        print(f"     ë³µí•©ì ìˆ˜: {path['composite_score']:.3f}")
                    if 'rediscovery_score' in path:
                        print(f"     ì¬íƒìƒ‰ì ìˆ˜: {path['rediscovery_score']:.3f}")
                    if 'agent_source' in path:
                        print(f"     Agent: {path['agent_source']}")
            else:
                print("âŒ ê²½ë¡œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ í…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


def show_workflow_structure():
    """ì›Œí¬í”Œë¡œìš° êµ¬ì¡° ì¶œë ¥"""
    print_langgraph_structure()
    
    


if __name__ == "__main__":
    import asyncio
    import sys
    
    print("ğŸ¯ LangGraph ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ë„êµ¬")
    print("="*60)
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "structure":
            show_workflow_structure()
        elif command == "test":
            asyncio.run(test_langgraph_workflow())
        elif command == "interactive":
            asyncio.run(interactive_test())
        else:
            print("âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.")
            print("ì‚¬ìš©ë²•:")
            print("  python app/services/langgraph_service.py structure  # êµ¬ì¡° ì¶œë ¥")
            print("  python app/services/langgraph_service.py test       # ìë™ í…ŒìŠ¤íŠ¸")
            print("  python app/services/langgraph_service.py interactive # ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸")
    else:
        print("ì‚¬ìš©ë²•:")
        print("  python app/services/langgraph_service.py structure  # ì›Œí¬í”Œë¡œìš° êµ¬ì¡° ì¶œë ¥")
        print("  python app/services/langgraph_service.py test       # ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        print("  python app/services/langgraph_service.py interactive # ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸")
        print("\në˜ëŠ” ì§ì ‘ ì‹¤í–‰:")
        print("  python -c \"import asyncio; from app.services.langgraph_service import test_langgraph_workflow; asyncio.run(test_langgraph_workflow())\"")
