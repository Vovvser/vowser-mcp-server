import os
import hashlib

from typing import List, Optional
from openai import OpenAI
from app.models.path import PathStep
from dotenv import load_dotenv, find_dotenv

load_dotenv(find_dotenv())

def get_openai_client():
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("í™˜ê²½ë³€ìˆ˜ì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤!")
            return None
        return OpenAI(api_key=api_key)
    except Exception as e:
        print(f"OpenAI client ì´ˆê¸°í™” ì‹¤íŒ¨. Error: {e}")
        return None

client = None

# ì„ë² ë”© ìºì‹œ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
_embedding_cache = {}
_CACHE_MAX_SIZE = 1000  # ìµœëŒ€ ìºì‹œ í¬ê¸°

def _get_cache_key(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ ìºì‹œ í‚¤ ìƒì„± (í•´ì‹œ ê¸°ë°˜)"""
    return hashlib.md5(text.strip().encode()).hexdigest()

def _clean_cache_if_needed():
    """ìºì‹œ í¬ê¸°ê°€ ìµœëŒ€ì¹˜ë¥¼ ì´ˆê³¼í•˜ë©´ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì‚­ì œ"""
    global _embedding_cache
    if len(_embedding_cache) > _CACHE_MAX_SIZE:
        # ê°„ë‹¨í•˜ê²Œ ì „ì²´ ìºì‹œì˜ ì ˆë°˜ ì‚­ì œ (FIFO)
        keys_to_remove = list(_embedding_cache.keys())[:_CACHE_MAX_SIZE // 2]
        for key in keys_to_remove:
            del _embedding_cache[key]
        print(f"ğŸ§¹ ì„ë² ë”© ìºì‹œ ì •ë¦¬: {len(keys_to_remove)}ê°œ í•­ëª© ì‚­ì œ")

def generate_embedding(text: str) -> Optional[List[float]]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜ (ìºì‹± ì§€ì›)
    
    Args:
        text (str): ì„ë² ë”©í•  í…ìŠ¤íŠ¸
        
    Returns:
        List[float] | None: ì„ë² ë”© ë²¡í„° ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    global _embedding_cache
    
    if not text or not text.strip():
        print("Warning: ì„ë² ë”© ìƒì„± ê±´ë„ˆëœ€: ë¹ˆ í…ìŠ¤íŠ¸ê°€ ì œê³µë˜ì—ˆìŠµë‹ˆë‹¤.")
        return None
    
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = _get_cache_key(text)
    
    # ìºì‹œì—ì„œ í™•ì¸
    if cache_key in _embedding_cache:
        print(f"ğŸ’¾ ì„ë² ë”© ìºì‹œ íˆíŠ¸: '{text[:30]}...'")
        return _embedding_cache[cache_key]
    
    # ìºì‹œ ë¯¸ìŠ¤ - ìƒˆë¡œ ìƒì„±
    client = get_openai_client()
    if not client:
        print("Warning: ì„ë² ë”© ìƒì„± ê±´ë„ˆëœ€: OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=text.strip()
        )
        embedding = response.data[0].embedding
        
        # ìºì‹œì— ì €ì¥
        _embedding_cache[cache_key] = embedding
        _clean_cache_if_needed()
        
        print(f"ğŸ“ ì„ë² ë”© ìƒì„± ë° ìºì‹±: '{text[:30]}...'")
        return embedding
    except Exception as e:
        print(f"Error: ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def create_embedding_text(step: PathStep) -> str:
    """
    PathStep ê°ì²´ì—ì„œ PAGE ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìƒì„±
    
    Args:
        step (PathStep): ê²½ë¡œ ë‹¨ê³„ ë°ì´í„°
        
    Returns:
        str: ì„ë² ë”©ìš©ìœ¼ë¡œ ì¡°í•©ëœ í…ìŠ¤íŠ¸
    """
    if not step.semanticData:
        return ""
    
    texts = []
    
    # textLabels ì¶”ê°€
    if step.semanticData.textLabels:
        texts.extend(step.semanticData.textLabels)
    
    # contextText ì¶”ê°€
    if step.semanticData.contextText:
        context = step.semanticData.contextText
        
        # immediate context
        if context.get('immediate'):
            texts.append(context['immediate'])
        
        # section context
        if context.get('section'):
            texts.append(context['section'])
        
        # neighbor context
        if context.get('neighbor') and isinstance(context['neighbor'], list):
            texts.extend(context['neighbor'])
    
    # pageInfo ì¶”ê°€
    if step.semanticData.pageInfo and step.semanticData.pageInfo.get('title'):
        texts.append(step.semanticData.pageInfo['title'])
    
    filtered_texts = [text for text in texts if text and text.strip()]
    return ' '.join(filtered_texts)
